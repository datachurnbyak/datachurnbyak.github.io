[
    {
    "title"    : "Simple Linear regression: Intuitive way"
    ,"category" : "statistics"
    ,"tags"     : "regression"
    ,"url"      : "/posts/2022-03-03-Simple-linear-regression"
    ,"date"     : "Mar 2, 2022"
    ,"content"  : "Simple Linear Regression This is a statistical method to establish a linear relationship between two quantitative variables such that one variable can give an estimate of the other. The variable being estimated is called dependent variable and the variable used to estimate is called independent variable or regressor. The first step in linear regression is to find a line of best fit. You may ask why do we want to fit a line, and not circle or a quadratic function? Well it has to do with one of the most important assumption of linear regression which states that there is a linear relationship between dependent variable and independent variable thus a linear regression with a single predictor can be represented as a equation for a line: \\[y = \\beta_0 + \\beta_1x + \\epsilon\\] where, \\(y\\) = dependent variable \\(\\beta_0\\) = y-intercept of the predicted line and the value of dependent variable when value of predictor variable \\(x\\) is 0. \\(\\beta_1\\) = slope of the line of fit or the coefficient by which predictor variable needs to be adjusted to get the predicted value of y (\\(\\hat{y}\\)). \\(\\epsilon\\) = there is always some difference between predicted value and the actual value of y which is called error term The variance of error term is equal for all observation and denoted by \\(\\sigma^2\\). Lets make a small dummy data where we want to predict the weight of 5 students by their height. click for code height = c(5.5, 3.4, 4.4, 6) weight = c(140, 120, 145, 170) par(mfrow=c(1,2)) plot(weight~height, col = \"black\" , cex = 2, pch =20, main = \"a\") #plot(weight~height, col = \"red\" , cex = 2, pch =20, ylim = c(0,170), xlim= c(0,6)) plot(weight~height, col = \"black\" , cex = 2, pch =20, main =\"b\") abline(h=143.75, col=\"red\") abline(98, 10, col= \"blue\") abline(70.287, 15.225, col= \"purple\") abline(60, 16, col= \"green\") abline(25,25, col =\"brown\") abline(-25,35, col =\"darkcyan\") done! Fig 1: Simple linear regression As seen in the scattered plot (figure 1.a), there appears to be some form of linear relationship between height and weight and we are very tempted to draw a line right across the points. Lets draw some potential lines of different color (Figure 1.b) that could be a best line of fit. Now how do we find which is the best line of fit? There are several algorithms that targets to minimize the sum of square of deviation, absolute deviation, lack of fit, or penalty for a cost function (example ridge regression and lasso). We will focus to well known and most used approach called least squares where the goal is to minimizing the sum of square of deviation between predicted and observed value of y. Lets implement the least squares in the 6 lines that we have drawn on our plot (Figure 1.b). To calculate the sum of square of deviation (SS) for each line defined by equation \\(y=a+bx\\), we need to first predict the value of y for given x. Once we have the predicted values, we can find the differences between observed and predicted value (aka deviation or residual or error in estimate) for all the data points. Followed by calculating square of deviation and adding them up to get sum of squares. Once we have SS for each line, we can plot those SS by lines to visualize the line with least SS. See code below for detail of the steps. click for code #store each line as a vector of a and b, y=a+bx red = c(143.75, 0) blue = c(98, 10) purple = c(70.287, 15.225) green = c(60, 16) brown = c(25, 25) darkcyan = c(-25, 35) lines &lt;- list(red=red, blue=blue, purple=purple, green=green, brown=brown, darkcyan=darkcyan) #write a for loop which will take a line at time and give prediction of weight for each height #first make a empty list to store predictions predicted_weight = vector(mode= \"list\", length=6) names(predicted_weight) &lt;- names(lines) # take a lines at a time and apply that line equation to all height values for(line in names(lines)){ predicted_weight[[line]] &lt;-sapply(height, function(x){lines[[line]][1] + lines[[line]][2]*x}) } #Now that we have predicted weight value from each line lets first plot par(mfrow= c(1,2)) plot(weight~height, col = \"black\" , cex = 2, pch =20, main=\"a\") for(line in names(lines)){ abline(lines[[line]], col=line) points(height, predicted_weight[[line]], col=line, pch=20, cex=1) } #Also calculate the SS for each lines deviation &lt;- lapply(predicted_weight, function(x) x-weight) #Square and sum each item in deviation SS &lt;- lapply(deviation, function(x) sum(x[1]^2,x[2]^2,x[3]^2,x[4]^2)) ##plot SS by the line barplot(unlist(SS), col= names(lines), ylab = \"Sum of squares\" , main=\"b\") text(x= 1:6, y= unlist(SS)-100, label= round(unlist(SS),1), cex = 0.6) done! Fig 2: Manually calculating least sum of square Figure 2.a shows the predicted points by each line as the colored dots in the respective line. And in the barplot on the right we can see the purple line has least sum of squares and is the best line of fit among these six lines. Ordinary Least Square In the above example we worked with only six lines and we could easily find the best one minimizing the sum of squares, but the extent of actual problem is much complex because there could be infinite number of lines passing this data. It is essentially a minimization problem and fortunately it is a well known and worked out problem in the field of mathematics. It is solved using the ordinary least square (OLS) method. Under the hood OLS uses principles of calculus, statistics and some assumptions to give an estimate of \\(\\beta_0\\) and \\(\\beta_1\\) denoted as \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) respectively for the line of best fit. Although we may skip the details of how this method works, we need to be clear about how to interpret them and what are their limitations. Following are the equation to calculate \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). \\[\\hat{\\beta_1}=\\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2}= \\frac{S_{xy}}{S_{xx}}\\] \\[\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\] Intuitive explaination of \\(\\hat{\\beta_1}\\) \\(\\hat{\\beta_1}\\) is the amount by which a unit change in independent variable will make the corresponding change in dependent variable. If \\(\\hat{\\beta_1}\\) is negative than a unit increase in x will decrease y by \\(\\hat{\\beta_1}\\). The above equation for \\(\\hat{\\beta_1}\\) can also be written as below: \\[\\hat{\\beta_1}= \\frac{\\text{Sample Covariance between x and y}}{\\text{Sample Variance of x}}\\] Which means higher the covariance between x and y higher is the value of \\(\\hat{\\beta_1}\\) and it is inversely proportional to the variance of independent variable. y-intercept These equations may look very technical but have some intuitive features. Usually \\(\\hat{\\beta_0}\\) is put first but, it is derived from \\(\\hat{\\beta_1}\\) and it is a constant needed to make the adjustment to the line equation so that our line of best fit passes through the point \\((\\bar{x},\\bar{y})\\). In absence of this intercept term, we force the line to pass through origin (0,0) and this may not be good fit for the data. Therefore we should always include this intercept term in regression equation. \\({\\beta_0}\\) ensures the line of best fit is passing through the center of the data \\((\\bar x, \\bar y)\\) and sum of all the residuals (difference between estimated y and observed y) is zero. Other than this, y-intercept does not have any value in explaining the relationship between x and y. Although we get p-value and standard error for the intercept because of mathematics behind OLS (and statistical software like R report it), we should not be tempted to force any interpretation to our data using the intercept. Theoretically, it is the estimate of y when all the other independent variables have value of 0 (no effect of any independent variable at all). In most cases, it is impossible to have independent variable to be zero (example its impossible to have a student with zero height). Even if your data is centered around origin (for example predicting amount of snow by temp in winter season), the value of y at the intercept is value of dependent variable when there is no independent variable, which is against our sole purpose which is to understand the effect of independent variable on dependent variable. Intercept should always be taken only as a factor used to adjust line of best fit and nothing more than that. In addition, these estimates have some very useful statistical properties: 1. Both \\(\\hat{\\beta_0}\\), and \\(\\hat{\\beta_1}\\) are an estimate of population \\({\\beta_0}\\) and \\({\\beta_1}\\) using the current sample of x and y. For example, Instead of just working with 4 students’ data as in our example, we can have data from 1000 of students, sampling 10 at a time and for each sample calculate \\(\\hat{\\beta_0}\\), and \\(\\hat{\\beta_1}\\). Doing so for multiple of times (1000 times) will give us the sampling distribution of \\({\\beta_0}\\) and \\({\\beta_1}\\). click for code library(MASS) set.seed(566) d &lt;- mvrnorm(n=1000, mu= c(5,143.75), Sigma = matrix(c(1, 0.6,0.6,1),nrow = 2)) heightPop &lt;- d[,1] weightPop &lt;- d[,2] par(mfrow=c(2,4)) sample_colour &lt;- rainbow(6) six_beta_0 &lt;- c() six_beta_1 &lt;- c() for (i in 1:6) { index &lt;- sample(1:1000, 10) s_wt &lt;- weightPop[index] s_he &lt;- heightPop[index] plot(weightPop~heightPop,col = \"grey\" , cex = 2, pch =20, main= letters[i], xlab=\"Height\", ylab=\"Weight\" ) points(s_he, s_wt, col= sample_colour[i], cex=2, pch=20 ) m &lt;- lm(s_wt~s_he) abline(m$coefficients, col= sample_colour[i]) text(mean(s_he),140.4,labels =paste0(\"y= \",round(m$coefficients[1],1), \" + \", round(m$coefficients[2],1), \"x\")) six_beta_0[i] &lt;- m$coefficients[1] six_beta_1[i] &lt;- m$coefficients[2] } beta_0 &lt;- c() beta_1 &lt;- c() for (i in 1:1000) { index &lt;- sample(1:1000, 10) s_wt &lt;- weightPop[index] s_he &lt;- heightPop[index] m &lt;-lm(s_wt~s_he) beta_0[i] &lt;-m$coefficients[1] beta_1[i] &lt;- m$coefficients[2] } hist(beta_0, breaks =100, main = \"Distribution of beta estimates\") for(i in 1:6)abline(v=six_beta_0[i], col=sample_colour[i]) hist(beta_1, breaks = 100, main = \" (both sampled 1000 times)\") for(i in 1:6)abline(v=six_beta_1[i], col=sample_colour[i]) done! Fig 3: Distribution of regression coefficient From the plots which shows distribution of 1000 \\(\\hat{\\beta}\\)s values we can see that it is normally distributed hence we can apply central limit theorem to these coefficients as well. Which gives that, the mean of such distribution of \\(\\hat{\\beta}\\)s is the unbiased estimate of \\({\\beta}\\)s and the variance of such distribution of \\(\\hat{\\beta}\\)s is given by OLS as follows: \\[Var(\\hat{\\beta_1)}= \\sigma^2\\frac{1}{S_{xx}}\\] \\[Var(\\hat{\\beta_0)}= \\sigma^2 \\left( \\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}\\right)\\] Where: \\(\\sigma^2\\) is the variance of residuals or error terms. These variance terms will be used for performing t-test for each coefficients (will see later). Assumptions OLS gives us estimate of coefficients and great statistical power but there are some limitations. For all the estimates to be valid, following assumptions should be fulfilled: - The population under study should strongly follows linear model and the parameters correctly specified. - The samples were taken randomly from the population (this is the single assumption necessary for distribution of sample mean). - There is variation in explanatory variable. - Homoskedasticity : The variance of residuals (\\(\\sigma^2\\)) is equal for all observation. The deviation we can calculate for each observation of y, \\(\\hat{e}\\) called residual is a random variable which follows normal distribution, and have mean \\(e\\) and variance equal to \\(\\sigma^2\\). Similar to \\(\\hat{\\beta}\\)) if we sample 1000 times and see the distribution of residual for each observation, it will have normal distribution and the variance needs to be same for all observations. - The error term is independent of the independent variable. Analysis of variance One we have the line of best fit, we can do inference about the statistical significance of the coefficients. Regression when we go back to basics is also an analysis of variance just like ANOVA and the only difference is, in ANOVA we want to find variance explained by a effect and make statistical testing .See the details in ANOVA. But, in regression we will first find a line of best fit and find variance explained by the line and do statistical testing to see if the variance explained by the line is significantly different than the unexplained variance. Lets plot our example of four sample and best line of fit we found for it. click for code plot(weight~height, col = \"black\" , cex = 2, pch =20) abline(70.287, 15.225, col=\"purple\") abline(h=143.75, col= \"red\") points(height, predicted_weight[[3]], col=\"purple\", pch=20, cex=1.5) done! In absence of any independent variable, mean of y \\(\\bar{y}\\) is the best estimate of y and total variance is calculated by taking deviation of observed values from \\(\\bar{y}\\). This is called Total Variance. We can estimate this by first calculating sum of square of deviation (TSS). Adding squares of all the red colored deviations in above figure will give TSS. Just like a normal variance of a sample this TSS has n-1 degree of freedom. \\[\\hat{Var(Total)}= MST= \\frac{SST}{n-1}\\] When we have the regression line some of the total variance is explained by the line, which can estimated by calculating the Sum of square of Regression (SSR). SSR is calculated by by adding up the square of deviation between the estimated value of y and mean of y. The sum of square of purple colored deviations in the above figure is equal to the SSR. The degree of freedom for SSR is equal to the number of coefficients in model - 1 (details in degree of freedom) . It is equal to the Sum of Square Between (SSB) calculated in ANOVA. The estimate of variance explained by regression is mean of SSR (MSR). \\[\\hat{Var(Regression)}= MSR= \\frac{SSR}{\\text{no. of } \\beta s - 1}\\] There is always a second component of variance called variance of error (\\(\\sigma^2\\)), which represents the variance not explained by regression line. It is estimated using Sum of Square of Error (SSE). Sum of square of deviation between estimated value of y and observed value of y is equal to SSE. In the figure above, sum of square of all black deviations is equal to SSE and is equal to the Sum of Square within (SSW) in ANOVA. It has degree of freedom equal to n - no. of \\(\\beta\\)s and mean of SSE (MSE) is given by following equation. \\[\\hat{Var(Error)}= \\hat{\\sigma ^2}= MSE= \\frac{SSE}{n - \\text{no. of } \\beta s }\\] Once we have MSR, and MSE we can do a F-test similar to the one in ANOVA. \\[F = \\frac{MSR}{MSE}\\] Equivalent t-test for coefficients It is also possible to statistically test if each of the coefficients used in the model is equal to zero or not. OLS provides necessary assumptions and properties to conduct a T-test on coefficients. \\[H_0 : \\beta _ 1 = 0\\] \\[H_1 : \\beta _ 1 \\neq 0\\] Although, I am showing for \\(\\beta _1\\) same principle applies to all the other coefficients in the model. z-statistic can be calculated using following equation: \\[z =\\frac{ \\hat{\\beta_1}-0}{\\sqrt{\\frac{\\sigma^2}{Sxx}}}\\] Because we only have the estimate of \\(\\sigma^2\\), which is equal to MSE, we can substitute that but our test statistic will now follow t-distribution. \\[t= \\frac{ \\hat{\\beta_1}-0}{\\sqrt{\\frac{MSE}{Sxx}}}\\] This t-statistics will have the same degree of freedom as MSE ie n - no. of \\(\\beta\\)s and the term \\(\\sqrt{\\frac{MSE}{Sxx}}\\) is called Standard Error in measure of \\(\\hat{\\beta_1}\\) Lets now compare the result of anova and manually calculated values aov(weight~height) ## Call: ## aov(formula = weight ~ height) ## ## Terms: ## height Residuals ## Sum of Squares 938.2682 330.4818 ## Deg. of Freedom 1 2 ## ## Residual standard error: 12.85461 ## Estimated effects may be unbalanced REF 1 2"
  },{
    "title"    : "Python data structures (compared to R)"
    ,"category" : "python"
    ,"tags"     : "dictionary, python data structures"
    ,"url"      : "/posts/2022-02-05-Python-Data-Structures"
    ,"date"     : "Feb 4, 2022"
    ,"content"  : "Learning Python for R users Comparative view of R and Python R and python are both extensive used in the field of data science and bioinformatics. Different people start learning programming from either languages. So it will be very helpful to see what are the common data structures in both languages so that you don’t have to invent the wheel again and again. Data Structures Both the languages have their own set of data formats to represent similar kind of data. It is important to understand the subtle differences between them. Data structures in R Data structures available in base R were written in the early phase during the evolution of R in S language. Although they are frequently called ‘Object’ are not the true objects in the light of object oriented programming (OOP). All R objects follow copy-on-modify semantics, so if same data is linked to two variable names and we make modification to one of the names, a new copy with modification is made for that variable name. When there is only one variable name bound to a data, R works by modify-in-place semantics. So, all the objects in R are mutable but because of copy-on-modify semantics we will not have any issue of indirectly modifying unwanted variables. The base R objects can be categorized as follows: Vectors Vectors are the flat data structures (1D array) which can have zero to multiple number of items (except NULL). Therefore, can be empty, single item or sequence. Vector objects can hold only values and in maximum one attribute(name). Atomic: These are the vectors of numbers, or string of characters and NULL which can hold only one type of data in one object. In R there are four types of atomic vectors: integer : These are the vectors of integers. double : These are the vector of decimal numbers.\\ logical : These are the vector of 0 and 1 or FALSE and TRUE. character : These are the vector of character strings. Recursive: These are the vector of one or more atomic vectors and it can contain data type of itself. Only recursive datatype available in R is list. At user level it is similar to the tuple in python. Non-vectors When a vector has additional attribute (in addition to name), it is no more called a vector because those attributes significantly alter how the values contained in the object is treated by R. Array The attribute, dim determines the shape of the vector contained. setting the dim to c(i,j) makes a vector behave like a matrix with i rows and j columns. S3 objects The attribute, class makes any R object, a object of that class and the class of a object will govern how a generic function will treat that object. The R base type of a S3 object could be any one of vector types (integer, double, logical, character or list). For example: factor is a S3 object with class factor and another attribute levels but the values it hold is all integer. Similarly, data frame is a S3 object with class data.frame but the R data type under the hood is list. S4 objects The S3 objects works in same principle as the S3 objects but have strict construction requirements to reduce the error down the pipeline. These R objects are described in detail in another post. None of these R objects are true OOP objects and do not hold any methods inside the object. Data structures in Python Python is a more general purpose language (has wider range of applications than just data science) therefore offers wider range of data types and because of its wider range of applications objects used in python are not as specialized for data analysis as R. The additional modules like numpy and pandas are required to explore the full potential of python for data analysis. Python follows strict object oriented programming hence all the data structures are true objects defined in OOP. The built in function type() gives the class of an instance of a object. The function isinstance() gives if an object is an instance of a specific class. It is also true if the class being tested is parent to the current class. Another attribute cls.__mro__ gives name of class and all parent classes in the method resolution order. Let us explore the built in data structures in python using these functions. 1. Scalars This includes built-in objects which store only single value and are not iterable and subscriptable. In R, any object created (except NULL) is a vector, ordered, and subscriptable. An identifier holding a single value is treated same as a vector of length one. But in python, objects of class float, int, bool, and NoneType when outside any container are not iterable and subscriptable. These are called scalars and has no length (len() function is not defined for these objects) NoneType: This is a special class which holds now value. It is simply a place holder. It has same function as NULL in R and only value this class can take is ‘None’ which Python interpreter takes as absence of value.\\ float: These are the numbers with values after decimal. It corresponds to ‘double’ in R and all syntax applicable to double in R are also applicable to float. int: These are ‘integers’ in R and follows the syntax used for syntax in R bool: These are ‘logical’ in R, while in R it can take value of TRUE/FALSE/T/F in python boolean can take value of True/False/1/0 only (case sensitive). Python class-wise, it is a child class of ‘int’. #None object a_none = None #create an instance of class NoneType type(a_none).__mro__ #get class and parent class to that object ## (&lt;class 'NoneType'&gt;, &lt;class 'object'&gt;) dir(a_none) #get all attributes of that object #len(a_none) #gives error ##Float - Decimal object ## ['__bool__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__'] a_float = 34.56 #create an instance of class float type(a_float).__mro__ #get class and parent class to that object ## (&lt;class 'float'&gt;, &lt;class 'object'&gt;) a_float.as_integer_ratio() #one of the example method in class float. #help(float) ##see details of all attributes of that class #len(a_float) #gives error ## (607985949695017, 17592186044416) a_bool = True type(a_bool).__mro__ #shows that bool is child class to int. ## (&lt;class 'bool'&gt;, &lt;class 'int'&gt;, &lt;class 'object'&gt;) Scalars in R are treated as vectors of length 1 R_none &lt;- NULL length(R_none) ## [1] 0 R_double &lt;- 34.56 R_double[1] #subscriptable ## [1] 34.56 length(R_double) #has length of 1 #length function is defined ## [1] 1 2. Containers These data structures as name suggests are containers with hold zero, one or more than one of the scalars discussed above, character strings, or container object itself. All containers are iterable ie can iterated one item at a time using the for statement. There are further 3 types of containers: a. Sequences: These are the containers where position of items in the sequence has value and an integer(starting from 0) can be used to extract an item from a desired position. There are three classes of objects in Python that fit this definition: - list : Sequence of any datatype of same kind or mixed is called a list. It is identified by square brackets [ ] and subset able using position similar to R with three major differences. i. Python list don’t have ‘name’ attributes. ii. Python list are mutable (values are modified in place even when multiple identifiers are pointing to same list) iii. Python list positions starts from 0 unlike 1 in R. #mutable nature of list in python a_list = [1,2,3,4,5] b_list = a_list #both points to same list b_list[0] = \"new Item at first\" a_list[0] #this change is reflected in a_list as well. ## 'new Item at first' #copy on modification semantics of R aR_list = c(1,2,3,4,5) bR_list = aR_list bR_list[1] = \"new Item at first\" aR_list[1] #this change doesn't affect aR_list . ## [1] 1 bR_list[1] #bR_list is modified ## [1] \"new Item at first\" -str : These class hold character string data in Python. Unlike R, a string of character is a subscriptable even without items being separated by a coma. a_string = \"This a string.\" len(a_string) ## 14 a_string[7:13] #last position is exclusive, start from 0 ## 'string' aR_string = \"This a string.\" length(aR_string) ## [1] 1 aR_string[8:14] #Gives NA because for R there is just 1 position ## [1] NA NA NA NA NA NA NA #need to use substr function substr(aR_string, 8,14) #last position is inclusive, start from 1 ## [1] \"string.\" tuple : This class is similar to list except that it is immutable and don’t have methods like append, copy, extend, insert, pop, remove. and reassignment of values. It only has count and index methods that can be used to count the occurrence of a value and index to return the index of first occurrence of a value. a_tuple = (1,2,3,4,5) #b_tuple = a_tuple. \"a new value\" #gives error no reasssignment. b. Sets: Sets are unordered collection of unique items and can contain any data types that are hashable (to identify the uniqueness). There are two types of sets based on if they are mutable or immutable: set and forzenset respectively. They are created using built-in functions set() and frozenset(). a_set = set([1,1,1,1,2,3,4,4,5]) a_set ## {1, 2, 3, 4, 5} a_set.pop() #removes first item ## 1 a_set #a_set[0] #gives error not subscriptable ## {2, 3, 4, 5} a_frozenset = frozenset([1,1,1,1,2,3,4,4,5]) a_frozenset #a_frozenset.pop() #gives error #are not mutable ## frozenset({1, 2, 3, 4, 5}) c. Dictionaries: This is a collection of key:value pairs where key should be unique and non-mutable but values can have any datatype and are mutable too. This is very similar to named list in R but names of items in list in R doesn’t need to be unique. a_dictionary = {'name': 'Sam', 'position': 'Manager', 'Address': 25, 'score': 90} a_dictionary['name'] #Subscriptable by key ## 'Sam' a_dictionary['name'] = \"Peter\" #values are mutable a_dictionary ## {'name': 'Peter', 'position': 'Manager', 'Address': 25, 'score': 90} for key in a_dictionary: print(a_dictionary[key]) ## Peter ## Manager ## 25 ## 90 Numpy Array These basic data structures offers very minimum resources for applications in data science so additional data structures and functions are offered by packages like NumPy and Pandas. One of the most important data structures provided by Numpy and used extensively in other packages like pandas, Scikit-Learn etc. Details on numpy available from numpy website. import numpy as np a_array = np.array([1,2,3,4,5,6]) # one dimensional array a_array.size ##gives length of array ## 6 a_array.ndim ##gives number of dimensions ## 1 a_array.shape ##gives dimensions ## (6,) a_array.reshape(3,2) #makes 3 rows 2 columns (2 dim array) ## array([[1, 2], ## [3, 4], ## [5, 6]]) Basic operations in R Vs Python. Using Functions In R, all functions are objects on their own and syntax function_name(arguments...) is used universally to apply a function. Python has diverse type of functions and syntax for using functions. Methods: Any objects in Python being a true object (in OOP context) can store functions within itself as an attribute. Such functions are called methods. Syntax for using methods is instance.method(addn_arguments...).When used like this, that particular object is automatically supplied as a first argument to that method. To check available methods for a class, in-built functiondir(class) or help(class) can be used. As we will see below methods suffixed and prefixed with __ are called as magic/dunder and will be called by other built-in functions. a_float = 34.5 a_float.as_integer_ratio() #using a method \"as_integer_ratio\" available in the class float ## (69, 2) dir(float) ## ['__abs__', '__add__', '__bool__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getformat__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__int__', '__le__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__pos__', '__pow__', '__radd__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmod__', '__rmul__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__set_format__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', 'as_integer_ratio', 'conjugate', 'fromhex', 'hex', 'imag', 'is_integer', 'real'] Built-in Functions: Python interpreter comes packed with few in-built functions. These functions belong to class “built-in_function_or_method”. The syntax for using these functions is similar to that of R (function(argument)) where argument is an instance of a class/class name. It relies on the dunder methods defined in the argument class. For example built-in function “abs()” is applicable to numbers only which have dunder “abs” defined. If we make a custom class of our own and define the abs then we can use built-in abs() to an instance of that class as well. Hence the built-in functions work just as a modulator directing to appropriate method in the class on which function is being applied. Mathematical operators like addition, subtraction all work by using these dunders. abs(-34.6) #abs function applied to an instance of float #help function takes name of a class #help(str) #make a custom class with abs defined ## 34.6 class A_dummyClass: def __abs__(self): return(\"This dummy object is now positive.\") abs(A_dummyClass()) #apply abs function to an instance of this dummy class ## 'This dummy object is now positive.' A_dummyClass().__abs__() ##calling it as a method works but is not a pythonic way ## 'This dummy object is now positive.' Functions of class ‘function’ The functions created using def outside of a class (not a method) are a special class of type “function” aka “first class objects”. They can be used similar to the built-in function in the syntax function_name(argurments...) and behave very similar to a user defined function in R. # Define a function to print given string three times. def print3X(a_string) : return(print(a_string*3)) type(print3X) #check class of this function. ## &lt;class 'function'&gt; print3X(\"A sample string.\\n\") #application of this function. ## A sample string. ## A sample string. ## A sample string. References: For completely new users suggested read Python in a Nutshell by Alex Martelli, Anna Ravenscroft and Steve Holden https://automatetheboringstuff.com/"
  },{
    "title"    : "Objects in R"
    ,"category" : "r"
    ,"tags"     : "r-objects, s3 objects in r, s4 objects in r"
    ,"url"      : "/posts/2022-02-03-Objects-in-R"
    ,"date"     : "Feb 2, 2022"
    ,"content"  : "Objects in R R by design is a functional programming language hence the original objects designed to represent variables, functions, environments and various language components are different than the objects used in true Object Oriented Programs. True OOP-like objects S3, S4 were added in different time during the evolution of R and build upon the base R objects. The function typeof() gives the base-type of any objects (including S3, S4) in R. Another function mode() gives similar output but groups integers and double as numeric. There are 25 different types of base R objects but six are worth mentioning as they are used to represent variable/data set and most commonly encountered by R users. Logical: TRUE or FALSE / 1 or 0 Double: Numbers with decimal values Integer: Numbers without decimal values Character: strings NULL : No value List: List of any combination of above five types All the other data types we make in R like matrix, array, data.frame, tibble etc. are derived almost entirely from these 6 object types. But before going to that it is important clarify some terms. 1. Atomic object : If an object cannot hold its own object type inside itself it is called atomic object. For example: a vector of logical, double, integer, character and NULL. The values in atomic objects are stored in unique memory locations. Can be represented like the figure below: Fig 1: atomic Vector figure The function is.atomic() is always true for logical, double, integer, character and NULL objects even if they have higher dimensions or name attribute. NB: For each of the atomic object type these is a special value “NA”, which represent missing values and missing values can be identified by using function is.na(). a_logic &lt;- as.logical(sample(0:1, 12, replace=T)) #create a logical of length 12 a_double &lt;- rnorm(12) #create a double of length 12 an_integer &lt;- 1:12 #create an integer a_char &lt;- letters[1:12] #create a character a_null&lt;- NULL ##See what type of object they are: typeof(a_logic) ## [1] \"logical\" # typeof(a_double) # typeof(an_integer) # typeof(a_char) # typeof(a_null) ##See if they are atomic or not is.atomic(a_logic) ## [1] TRUE # is.atomic(a_double) # is.atomic(an_integer) # is.atomic(a_char) # is.atomic(a_null) ##They will remain as atomic even after adding higher dimensions dim(an_integer) &lt;- c(3,4) is.atomic(an_integer) #Still true ## [1] TRUE 2. Recursive object: These are the objects which can hold its own object type within itself. For example list can contain another list inside itself. It is represented by following figure. Fig 2: list object figure The opposite of function is.atomic() is is.recursive(). a_list &lt;- list(1:9, 2:4, \"a\", 3:5) is.atomic(a_list) #FALSE ## [1] FALSE is.recursive(a_list) #TRUE ## [1] TRUE Fig 3: Types of objects in R Attributes Attributes are the metadata associated with each R-objects. This metadata further controls how an object is processed inside R and how we perceive an object to be. One of the simplest attribute is name which gives a name to each value in a vector. Vector Any base R objects can be a vector (including a list) as long as it has no any attributes or single attribute name. Use the function attributes() to get all the attributes associated with a object and function attr() to get a specific attribute. Once other attributes like dim or class are added it is no more called a vector and the function is.vector() will be FALSE. attributes(a_double) # the vector we created has no attributes, gives NULL ## NULL is.vector(a_list) ##A simple list is also a vector ## [1] TRUE names(a_list) &lt;- c(\"list1\", \"list2\",\"list3\", \"list4\") a_list #print named list ## $list1 ## [1] 1 2 3 4 5 6 7 8 9 ## ## $list2 ## [1] 2 3 4 ## ## $list3 ## [1] \"a\" ## ## $list4 ## [1] 3 4 5 is.vector(a_list) #it is still a vector ## [1] TRUE Matrix Adding another attribute dim makes a vector (including a list although less common in practice) into a array. Matrix is a special case of array where we have only two dimensions (rows and columns). Matrix is treated as special because it is in the form in which many data are represented (a table with rows and columns.). Therefore there are some function in R that are specific to a matrix like (rownames(), colnames(), nrow(), ncol(), rbind(), cbind(), t()) which also applies to a data.frame in R. One major difference between matrix and data.frame is base data type of matrix is atomic type (character, integer, double …) but data.frame is stored as a list. Because of the nature of atomic objects, matrix build from atomic objects should have same datatype in all columns. dim(a_double) &lt;- c(3,4) #this makes a matrix out of variable a_double with 3 rows and 4 columns and filled columns first a_double ## [,1] [,2] [,3] [,4] ## [1,] 0.9822688 -0.07954649 1.62053229 -1.46717395 ## [2,] -0.7089867 -0.10584293 -1.47000813 1.81499311 ## [3,] -0.3611035 1.41759061 -0.03876332 0.04078724 is.vector(a_double) #It is no more a vector ## [1] FALSE is.atomic(a_double) #But is still a atomic ## [1] TRUE dim(a_list) &lt;- c(2,2) # a matrix of list can also be made but it won't look like a data.frame a_list ## [,1] [,2] ## [1,] integer,9 \"a\" ## [2,] integer,3 integer,3 Array When a object has 2 or more dimension as attribute it is called a array. dim(a_double) &lt;- c(2,3,2) a_double # The third dimension is shown by breaking the table, it can be regarded as two 2X3 tables ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 0.9822688 -0.36110347 -0.1058429 ## [2,] -0.7089867 -0.07954649 1.4175906 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 1.620532 -0.03876332 1.81499311 ## [2,] -1.470008 -1.46717395 0.04078724 Class and S3 object Attribute class is very important because it powers the whole OOP in R. Once a R base object has class attribute it is called a S3 object. Commonly used S3 objects are factors, dates, data and time, data.frame, and tibble. Because of this very lenient criteria, it is possible to assign any object to class which it should not belong to. For example we can assign a object of type double as data.frame. R will not stop us from doing this but since double will not be compatible with functions related to data.frame we will eventually run us into an error. Therefore we should always use a constructor function provided for a class to make a class instead of simply assigning a class. Any object with class attribute can be identified by function is.object(). a_double &lt;- rnorm(12) dim(a_double) &lt;- c(6,2) a_double #looks like a data.frame ## [,1] [,2] ## [1,] 0.10532213 -0.725692115 ## [2,] 0.63655714 -0.003428998 ## [3,] 1.82306620 1.132239738 ## [4,] 0.11533008 -0.055686044 ## [5,] -0.05375573 1.180911669 ## [6,] 0.22075057 -0.016582626 class(a_double) &lt;- \"data.frame\" ##assign a new class to object a_double attributes(a_double) # now we see a_double has class attribute which is data.frame ## $dim ## [1] 6 2 ## ## $class ## [1] \"data.frame\" class(a_double) #it says a_double has a class = data.frame ## [1] \"data.frame\" a_double[1,1] #hit an error trying to use a data.frame function ## NULL ## &lt;0 rows&gt; (or 0-length row.names) ##proper way a_double &lt;- rnorm(12) dim(a_double) &lt;- c(6,2) df &lt;- data.frame(a_double) #use the constructor function. df ## X1 X2 ## 1 0.4021199 0.57215428 ## 2 -0.9181690 0.71649773 ## 3 -0.2014391 1.98107538 ## 4 -1.5345385 2.00999835 ## 5 0.3284315 -0.64234315 ## 6 0.6641933 -0.01521207 class(df) ##now it is actually a data.frame ## [1] \"data.frame\" Factor Factor is one of the most used and also one of the most confused S3 object in R. It is build on top of the base R object integer and must have two attributes: class: must be “factor” levels: must be atomic vector of unique values in factor. Because it is a S3 object the generic function print will behave specific to factor and prints the factors at the position indicated by the integers in actual object. a_char &lt;- c(\"a\",\"b\",\"c\",\"a\",\"c\",\"b\",\"d\") typeof(a_char) ## [1] \"character\" print(a_char) ## [1] \"a\" \"b\" \"c\" \"a\" \"c\" \"b\" \"d\" as.numeric(a_char) ##No numeric is produced ## Warning: NAs introduced by coercion ## [1] NA NA NA NA NA NA NA a_factor &lt;- as.factor(a_char) print(a_factor) #now it is a factor so we see same character ## [1] a b c a c b d ## Levels: a b c d typeof(a_factor) #under the hood it is an integer ## [1] \"integer\" attributes(a_factor) ## $levels ## [1] \"a\" \"b\" \"c\" \"d\" ## ## $class ## [1] \"factor\" as.numeric(a_factor) #gives the underlying integer ## [1] 1 2 3 1 3 2 4 data.frame It is a S3 object which is build upon a list. Individual columns are stored like a individual items in a list. So data.frame is actually a list where all the items in list must have equal length. S4 object Because S3 objects are easy to incorporate errors another OOP type object called S4 was build which is extensively used by bioconductor community. Most of these S4 objects are build on base R object type list. library(edgeR) ## Loading required package: limma a&lt;-DGEList() ## Warning in min(lib.size): no non-missing arguments to min; returning Inf ## Warning in min(norm.factors): no non-missing arguments to min; returning Inf typeof(a) ##under the hood it is a list ## [1] \"list\" class(a) ##gives class \"DGEList\" ## [1] \"DGEList\" ## attr(,\"package\") ## [1] \"edgeR\" attributes(a) #shows all the attributes ## $class ## [1] \"DGEList\" ## attr(,\"package\") ## [1] \"edgeR\" ## ## $names ## [1] \"counts\" \"samples\""
  },{
    "title"    : "Essence of Matrix operations with Python"
    ,"category" : "math"
    ,"tags"     : "python, linear algebra, matrix, vector"
    ,"url"      : "/posts/2022-02-10-Matrix"
    ,"date"     : "Jan 31, 2022"
    ,"content"  : "Linear Algebra in Data Science Vectors and matrices are heavily used in computer science (and data science) to store and perform multiple mathematical operations on a data set.Although, the concept of vector as used in physics (example a quantity with both magnitude and direction (2 dimensional vector)) and vector used in computers (example list of features recorded on a observation) have very different meanings in real life, when performing mathematical operations, both of these vectors follow same rules. Therefore, the branch of discrete mathematics, that deal with manipulation of vectors has applications in diverse fields like computer programming, storing big data set, cryptography, image editing, graph theory and many more. Hence, a through understanding of fundamental concepts of vectors and matrix operation is necessary to understand multiple algorithms used in data science. In in this article, I am trying to highlight the importance of vectors and matrices in data science by showing some matrix and vector operations in Python. ( Code in R and data used are available here.) Vectors In data science, any list of values is a vector and as long as the manipulations on this list follows linear operations (not exponential or higher order transformations), the theories of linear algebra can be used to perform complex calculations. For example to store a 28X28 pixel gray scale image in a computer, we can make a 28*28=784 dimensional vector which will hold values for intensity of black at each pixel. Following code chuck in Python reads a csv holding such data for images of 150 handwritten letters one letter per row. ##load necessary modules import pandas as pd import numpy as np import matplotlib.pyplot as plt #Read handwriting data set Handwriting = pd.read_csv(\"DigitMatrix.csv\") print(\"Summary of DataFrame \" ) ## Summary of DataFrame Handwriting.describe() ## y X1 X2 X3 ... X781 X782 X783 X784 ## count 150.000000 150.0 150.0 150.0 ... 150.0 150.0 150.0 150.0 ## mean 4.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## std 2.953783 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## min 0.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## 25% 0.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## 50% 5.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## 75% 7.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## max 7.000000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 ## ## [8 rows x 785 columns] print(\"The first column is the label of these digits and below is the frequecy distribution \") ## The first column is the label of these digits and below is the frequecy distribution Handwriting['y'].value_counts() #Make a DataFrame excluding labels ## 7 50 ## 0 50 ## 5 50 ## Name: y, dtype: int64 Handwriting_matrix = Handwriting.iloc[:,1:786] print(\"View of a single vector in this data set\") ## View of a single vector in this data set Handwriting_matrix.iloc[1] ## X1 0 ## X2 0 ## X3 0 ## X4 0 ## X5 0 ## .. ## X780 0 ## X781 0 ## X782 0 ## X783 0 ## X784 0 ## Name: 1, Length: 784, dtype: int64 This view of first row in this handwriting_matrix shows a representation of familiar column vector which has 784 dimensions. #visualize some examples print(\"Visualization of some example images\") ## Visualization of some example images fig, (ax0, ax1, ax2) = plt.subplots(ncols=3) ax0.imshow((np.array(Handwriting_matrix.iloc[1])).reshape(28,28), cmap=\"binary\") ax1.imshow((np.array(Handwriting_matrix.iloc[55])).reshape(28,28), cmap=\"binary\") ax2.imshow((np.array(Handwriting_matrix.iloc[105])).reshape(28,28), cmap=\"binary\") fig.tight_layout() plt.show() The figure above shows three example images from 150 images present in this handwriting data. Each row in this dataframe can be considered a vector of 784 dimensions. Vector addition and scalar multiplication Lets consider a situation, How can we find average intensity of each pixel for each of these three digits ? Instead of finding aveage of one pixel at time, we can perform simple vector addition of all 50 vectors and divide the sum vector by the total number of images(50) of that category. This operations is just the extension to 784 dimensions of simple vector addition and scalar multiplication illustrated by 2 dimensional vectors below: \\[Sum(u,v)= \\begin{bmatrix}a \\\\ b \\end{bmatrix} +\\begin{bmatrix}c \\\\ d \\end{bmatrix}=\\begin{bmatrix}a+c \\\\ b+d \\end{bmatrix}\\] \\[k*u= \\begin{bmatrix}ka \\\\ kb \\end{bmatrix}\\] Vector addition and multiplication performed in Python to find average pixel intesity for each digit. #Find average of each pixel for each category of number Sum_Sevens= (Handwriting_matrix.iloc[0:50]).sum(0) Average_Sevens= Sum_Sevens*1/50 #Can also use mean function in pandas Average_zeros= Handwriting_matrix.iloc[50:100].mean(0) Average_fives= Handwriting_matrix.iloc[100:150].mean(0) #Visualize the average pixel for each category fig, (ax0, ax1, ax2) = plt.subplots(ncols=3) ax0.imshow((np.array(Average_Sevens)).reshape(28,28), cmap=\"binary\") ax1.imshow((np.array(Average_zeros)).reshape(28,28), cmap=\"binary\") ax2.imshow((np.array(Average_fives)).reshape(28,28), cmap=\"binary\") fig.tight_layout() plt.show() Vector subtraction Subtraction of vectors can be performed similar to vector addition, by subtracting each item of second vector from the first vector. \\[Subtraction(u,v)= \\begin{bmatrix}a \\\\ b \\end{bmatrix} -\\begin{bmatrix}c \\\\ d \\end{bmatrix}=\\begin{bmatrix}a-c \\\\ b-d \\end{bmatrix}\\] To see some examples of vector subtraction lets consider only one example of image from our 150 images above. A single digit image is composed of 784 pixels and location of each of these pixels in Cartesian coordinate can be represented by a 2d vector where the first row corresponds to x-coordinate and second row corresponds to y-coordinate. In order to completely describe each pixel we need to add a 3rd dimension to our pixel vector which will hold the value of intensity of black. Remember this 3rd dimension has nothing to do with the 3d dimension in space, vector dimensions in computer science keeps on increasing as new information is collected about that vector. For example if each of these pixels were RGB colored pixels in addition to x,y coordinates, there would be 3 more dimensions one each for R, G and B value, making each pixel a 5-dimensional vector. The code chuck below shows how a image can be represented by 874 vectors of dimensions = 3. #Take a image with row index = 105 SingleImage = np.array(Handwriting_matrix.iloc[105, :]) Vectors = pd.DataFrame({\"x\":np.tile(np.arange(1,29),28), # 1st dim is x-coordinate \"y\":np.repeat(np.arange(28,0,-1),28), #2nd dim is y-coordinate \"z\":SingleImage.ravel()}) #3rd dim is intensity of black Vectors.describe() #write a function to visualize these 784 vectors ## x y z ## count 784.000000 784.000000 784.000000 ## mean 14.500000 14.500000 26.001276 ## std 8.082904 8.082904 69.831479 ## min 1.000000 1.000000 0.000000 ## 25% 7.750000 7.750000 0.000000 ## 50% 14.500000 14.500000 0.000000 ## 75% 21.250000 21.250000 0.000000 ## max 28.000000 28.000000 255.000000 def DigitImage(df): fig, ax = plt.subplots() ax.grid() ax.axhline(y=0) ax.axvline(x=0) ax.scatter(\"x\",\"y\", data=df, c=\"z\", marker=\"s\", cmap= \"YlOrRd\", vmin=0, vmax=255) ax.set_xlim(left=-28, right=28) ax.set_ylim(bottom=-28, top= 28) ax.set_axisbelow(True) plt.show() DigitImage(Vectors) The figure above is generated by plotting 784 points (shaped square to mimic square pixels) and color intensity mapped to a color gradient from yellow to red (yellow= 0, red= 255). Hence each of the square is a vector of 3 dimensions. Now, if we want to move this image so that it is centered at origin we need to perform some vector subtraction. The vector \\(\\begin{bmatrix}14 \\\\ 14 \\\\249 \\end{bmatrix}\\) is the midpoint of this image (shown in the image below). If we subtract this vector from all 784 vectors the image should be centered at origin. fig, ax = plt.subplots() ax.grid() ax.axhline(y=0) ax.axvline(x=0) ax.scatter(\"x\",\"y\", data=Vectors, c=\"z\", marker=\"s\", cmap= \"YlOrRd\", vmin=0, vmax=255) ax.arrow(0,0,14,14, width=0.3, length_includes_head=True, color='black') ax.text(14,14, \" Vector [14,14,249]\", color=\"blue\") ax.set_xlim(left=-28, right=28) ## (-28.0, 28.0) ax.set_ylim(bottom=-28, top= 28) ## (-28.0, 28.0) ax.set_axisbelow(True) plt.show() The figure above show the pixel that occur at the center of this image as an arrow. Now the transformation of this image to the center can be represented as a vector subtraction as below: \\[\\text{Transform to origin}= \\begin{bmatrix}x \\\\ y \\\\z \\end{bmatrix} -\\begin{bmatrix}14 \\\\ 14 \\\\0 \\end{bmatrix}=\\begin{bmatrix}x-14 \\\\ y-14 \\\\z-0 \\end{bmatrix}\\] Centered = Vectors.apply(lambda x: (x) - [14,14,0], axis=1,) DigitImage(Centered) Matrix The simplest definition of a matrix is, it is collection of vectors. For example the identity matrix for a 2d vector is: \\[\\begin{pmatrix} 1 &amp; 0 \\\\0 &amp;1 \\end{pmatrix}\\] Here the first and second columns are simply the basis vector in x-direction and y-direction after the transformation by this matrix. Since this is matrix that does not bring about any transformation both columns are same as the original basis vectors of this system. The matrix multiplication with vector is formulated in such a way that when a matrix is multiplied with the transformation matrix it is transformed according the values of new basis vector coded in the matrix. Skew The following matrix can skew the image by 45 degree towards left without making any change to the 3rd dimension which stores information for color. \\[\\begin{pmatrix} 1 &amp; -1 &amp;0 \\\\0 &amp;1 &amp;0\\\\0&amp;0&amp;1 \\end{pmatrix}\\] The basis on x-direction is unchanged while for basis in y-direction now has a x-component of -1. ##write a function which takes df and matrix and gives transformed df def matrixTrans(Df, matrix): Transformed =pd.DataFrame(np.matmul(matrix, pd.DataFrame.to_numpy(Df).transpose() )) Transformed = Transformed.transpose() Transformed.columns = ['x','y','z'] return(Transformed) mat1=np.array([[1, -1, 0], [0, 1, 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Skew in y direction import math mat1=np.array([[1, 0, 0], [-1, 1, 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Rotation 45 counterclockwise import math mat1=np.array([[math.cos(45),-math.sin(45),0], [math.sin(45), math.cos(45), 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) #### Rotation 90 counterclockwise import math mat1=np.array([[math.cos(90),-math.sin(90),0], [math.sin(90), math.cos(90), 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Flip along y-axis mat1=np.array([[-1,0,0], [0, 1, 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Flip along x-axis mat1=np.array([[1,0,0], [0, -1, 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Decrease color intensity by 50% mat1=np.array([[1,0,0], [0, 1, 0], [0, 0, 0.5]]) VecTrans = matrixTrans(Centered, mat1) DigitImage(VecTrans) Matrix multiplication Matrix multiplication is simply a way to represent a sequence of transformation. For example \\(B.A\\) , simply represents apply transformation by A first and then B. This step by step transformation will give same result at apply transformation by BA. For example lets first flip the letter along y-axis and then apply skew left. #Flip mat1=np.array([[-1,0,0], [0, 1, 0], [0, 0, 1]]) #skew left mat2=np.array([[1,-1,0], [0, 1, 0], [0, 0, 1]]) VecTrans = matrixTrans(Centered, mat1) VecTrans2= matrixTrans(VecTrans, mat2) DigitImage(VecTrans2) #Flip mat1=np.array([[-1,0,0], [0, 1, 0], [0, 0, 1]]) #skew left mat2=np.array([[1,-1,0], [0, 1, 0], [0, 0, 1]]) comb = np.matmul(mat2,mat1) VecTrans = matrixTrans(Centered, comb) DigitImage(VecTrans) Inverse matrix This is the matrix which will revert the transformation caused by its complementary matrix. #inverse of comb matrix above is invMat = np.linalg.inv(comb) #apply this inv transformation to VecTrans Orig = matrixTrans(VecTrans, invMat) DigitImage(Orig)"
  },{
    "title"    : "Principle of inference"
    ,"category" : "statistics"
    ,"tags"     : "t-test, hypothesis testing, inference, null hypothesis, alternative hypothesis, distribution"
    ,"url"      : "/posts/2022-01-13-Principle-of-inference"
    ,"date"     : "Jan 12, 2022"
    ,"content"  : "Introduction Is the height of male (Age group 20-39 years old) in North America greater than 5.5 ? Is the average height of male (Age group 20-39 years old) in North America greater than 5.5 ? What is the average height of male (Age group 20-39 years old) in North America, Europe, and Asia ? Is the average height of male (Age group 20-39 years old) in North America different than in Europe or Asia ? We often see similarly formatted questions statistics and use the concepts of inference and hypothesis testing to solve them. Inference and Hypothesis testing are the two most important and the most confusing concepts in statistics. Inference is related to estimating the population parameters from a sample. Hypothesis testing as its name suggests is a two step process, first is to build a hypothesis and second is to test how well our data support that hypothesis. Inference and hypothesis testing are highly interdependent on each other because the need to build a hypothesis before testing rises because we had to use inference in the first place. Therefore, the best way to understand them is by learning them together but understanding the fine differences between each other at the same time. We will decipher this mystery by solving to questions listed above by two different approach. First approach is a hypothetical situation where we will not use any inference and study the whole population while in the second approach we will do orthodox sampling and hypothesis testing. Without using inference Population is a data set representing all the entity of interest. For example: If we are interested in study of distribution of facebook users by country then, all 2.912 billion facebook users (facebook data 2022) constitute the population. At the same time if we are interested in distribution of facebook users in USA by state, then the facebook users in USA (307.34 million) constitute the population. Similarly, if we are interested in effect of age on hypertension in human being, then every single human being in world (7.9 billion, 2022) constitute our population. In situations where we are interested in small population like comparing the performance of two high school senior students only, we can make measurement on all students and compare the mean performance score in each school but such study will have very narrow scope and that result cannot be generalized to any other schools. Measurements made on a population are called parameters and mean is denoted by \\(\\mu\\) and variance is denoted as \\(\\sigma^2\\). Now lets dive into our questions. If we are not going to use inference then we do not have our favorite option of sampling, we are forced to study the whole population. After some google search I could figure out that there are 450, 105 and 75 million males of this age group in Asia, Europe and North America respectively and each of these constitute a population for this question. Therefore, we will first need to find the height of each of these millions of people from each continent. We can simulate such population in R by generating a dummy population of such size and also calculate the mean, standard deviation and distribution for each population (Figure 1). Even for R, processing such large sized vector took quite a while, imagine you had to do this in reality. How much resource, money, time, and bookkeeping you would need to accomplish this? code here set.seed(578) Asia &lt;- rnorm(4.5e8, mean=5, 0.2) NorA &lt;- rnorm(7.5e+07, mean=6.2, 0.4) Euro &lt;- rnorm(1.05e+08, mean= 5.7, 0.3) mean_Asia &lt;- mean(Asia) var_Asia &lt;- var(Asia) hist_asia &lt;-hist(Asia, breaks = 200, plot=FALSE) mean_NorA &lt;- mean(NorA) var_NorA &lt;- var(NorA) hist_NorA &lt;- hist(NorA, breaks= 200, plot=FALSE) mean_Euro &lt;- mean(Euro) var_Euro &lt;- var(Euro) hist_Euro &lt;- hist(Euro, breaks= 200, plot=FALSE) plot(hist_asia, border=\"blue\", col=NULL, xlim = c(4.2,7.5), xlab = \"Continent\", main=\"Distribution of height of male aged 20-39 in 3 different continents\") plot(hist_NorA, border = \"red\", col=NULL, add=TRUE) plot(hist_Euro, border= \"darkgreen\", col=NULL, add=TRUE) legend(5.5,8e6, legend = c(paste0(\"Asia (mean = \",round(mean_Asia,1), \", sd= \",round(sqrt(var_Asia),3),\")\" ), paste0(\"Europe (mean = \",round(mean_Euro,1), \", sd= \",round(sqrt(var_Euro),1),\")\" ), paste0(\"North America (mean = \",round(mean_NorA,1), \", sd= \",round(sqrt(var_NorA),1),\")\" )), fill= c(\"blue\", \"darkgreen\", \"red\")) done! For now lets assume, R did all that hard work for us and we have necessary data we wanted to collect and we can focus on the answers to the question above: Is the height of male (Age group 20-39 years old) in North America greater than 5.5 ? We can see from figure 1 that most of the heights in North America is greater than 5.5. It may be tempting to answer “Yes” to this question but if we do so, we will be wrong because all the heights in North America are not above 5.5 (there is small proportion of heights in N America below 5.5). The alternative is to first calculate the probability of finding a height less than 5.5 (\\(\\alpha\\)) in N America and re-phrase our answer as “the height in N America is greater than 5.5 but the probability that this is incorrect is \\(\\alpha\\). The smaller this \\(\\alpha\\) aka significance level or p-value gets there is more certainty in our answer. The generally acceptable value of \\(\\alpha\\) is &lt; 0.05. Since, we already know mean and sd of all these three population we can plug these number in pdf of normal distribution in R to find these p-values and also visualize the probability we are calculating as the area under the curve (figure 2). For details about probability distribution and how to calculate probabilities for normally distributed random variable see my notes on normal distribution and binomial distribution). code here cat(\"Probability that heights in N. America is less that 5.5 is = \", pnorm(5.5, mean= mean_NorA, sd= sqrt(var_NorA)), \"\\n\") cat(\"Probability that heights in Europe is less that 5.5 is = \", pnorm(5.5, mean= mean_Euro, sd= sqrt(var_Euro)), \"\\n\") cat(\"Probability that heights in Asia is less that 5.5 is = \", pnorm(5.5, mean= mean_Asia, sd= sqrt(var_Asia)), \"\\n\") curve(dnorm(x, mean=5, sd=0.2), from = 4,to= 7.5, col= \"blue\", ylab= \"density\", xlab=\"height(in)\") polygon(c(seq(4, 5.5, 0.05),5.5), c(dnorm(seq(4, 5.5, 0.05), mean =5, sd=0.2),0), col=\"lightblue\") curve(dnorm(x, mean=5.7, sd=0.3), col= \"darkgreen\",add=T) polygon(c(seq(4, 5.5, 0.05),5.5), c(dnorm(seq(4, 5.5, 0.05), mean =5.7, sd=0.3),0), col=\"lightgreen\") curve(dnorm(x, mean=6.2, sd=0.4), col= \"red\", add=T) polygon(c(seq(4, 5.5, 0.05),5.5), c(dnorm(seq(4, 5.5, 0.05), mean =6.2, sd=0.4),0), col=\"red\") curve(dnorm(x, mean=5, sd=0.2), col=\"blue\", add = T) legend(5.6,2, legend = c(paste0(\"Asia (mean = \",round(mean_Asia,1), \", sd= \",round(sqrt(var_Asia),3),\")\" ), paste0(\"Europe (mean = \",round(mean_Euro,1), \", sd= \",round(sqrt(var_Euro),1),\")\" ), paste0(\"North America (mean = \",round(mean_NorA,1), \", sd= \",round(sqrt(var_NorA),1),\")\" )), fill= c(\"blue\", \"darkgreen\", \"red\")) done! ## Probability that heights in N. America is less that 5.5 is = 0.04006165 ## Probability that heights in Europe is less that 5.5 is = 0.2524402 ## Probability that heights in Asia is less that 5.5 is = 0.9937895 Using the p-values we can now give the complete answer; Yes, the height of male (Age group 20-39 years old) in N America is greater than 5.5 ft, given that this may be wrong 4% of the time. This 4% is below the generally accepted level of 5% error rate. Therefore, if a jeans manufacturer in N. America makes pants for height only above 5.5 ft, majority of man in N. America can find pants of their size. While a similar company in Europe would never want to manufacture pants that fit only above 5.5 ft because one fourth of its customers won’t be able to use that. Similarly, such company in Asia will manufacture pants that will fit heights less than 5.5 ft only. Therefore, although the answer we get using statistics may sound like incomplete information it has a lot of practical application when the result needs to be implemented in a large population. Lets review what we did here because it forms the foundation of any hypothesis testing. When the underlying distribution of a random variable (Y) is known, for any value of that random variable (y), we can find the probability of observing values more extreme than that value(y). If that probability is very small, then we can say, that value(y) itself is very rear in this distribution and could belong to other distribution. To take an example from the problem we are working with, we saw that probability getting a height greater than 5.5 in Asia is extremely low (0.00063). Therefore, if we randomly sample a man from the whole world and his height is found to be 5.5, then we can very confidently (probability of error is only 0.00063) say he is not from Asia. Does this mean he belong to N America? Or Europe? No, we will never know that answer but the point is when we properly select the random variable(Y) (sample mean, difference of sample mean, sample variance, sample proportion etc. ) and ask the correct question we can get some very useful answers about question of interest. Once we understand this we will later see how aptly early statisticians have devised hypothesis testing to get best out of it. Is the average height of male (Age group 20-39 years old) in North America greater than 5.5 ? Since we have made measurements on every man in that age group we can find the true mean by calculating sum of all the heights in a continent and divide by the population size of that continent which R already did for us. The mean height in N America is 6.2. This is true mean of this population so we do not need to make any estimate or confidence intervals to say that mean height in N America is greater that 5.5. What is the average height of male (Age group 20-39 years old) in North America, Europe, and Asia ? Similar to question no. 2, since we have complete population data, we can say with 100% confidence that mean height in N America, Europe and Asia is 6.2, 5.7 and 5 respectively. Is the average height of male (Age group 20-39 years old) in North America different than in Europe or Asia ? Answer to this is also straight forward. From question no 3 we can see clearly, they are different for these three populations. We saw, many of these questions could be easily answered because we could measure each and every man and get true mean \\((\\mu)\\) and sd \\((\\sigma)\\) for these populations. But recall again how arduous that task could be and the real-world situation is even more complicated by the fact that most populations we need to study have infinite numbers of entities, so it is impossible to find the true population parameters. This is where inference shines in. Inference provides the unbiased estimate of the population parameters using the information from a sample of certain size (n) from that population. Now, that we know the complications of studying the whole population we will see below, how we can deal with our question in much practical way using principles of inference and hypothesis testing, but first lets understand how inference works in general. Statistical inference sample is a subset of population (size = n) and is unbiased representation of the population. If n= \\(\\infty\\), sample behaves like the population and if n = 1, sample is just one observation from the population. It is very important for sample to be a representative of population because ultimately measurements made on a sample will be used to infer the population parameter. Any measurements made on a sample is called statistic (note statistics is branch of mathematics which study these statistic). For example sample mean (\\(\\bar x\\)) and sample variance (\\(s^2\\)) are two most important sample statistic. Sample is the main workhorse of statistics. This is all we have to make inference about population parameters. Usually a sample used to make inference on population parameters is much smaller in size than the population itself so it is very convenient to collect data (statistic) on a sample but this convenience comes at some cost, which is uncertainty associated with the estimate. We will see later, this uncertainty depend on population variance and we can tackle it by increasing sample size (n). Fortunately, in many real life situation, a sample of small size gives an estimate of population parameters with high precision. Therefore in a statistical analysis, sample statistic is used to make a statement about the population parameter which is always accompanied by a degree of precision (or chances of making an error). In situations where, the precision is relatively high (chances of making an error is low) we trust that statement and interpret our experiment based off that statement about population. On the other hand, if precision is low (chances of making an error is high), we do not trust that statement about population, so we cannot make any interpretation about the experiment or question of interest. The widely accepted level of that accuracy in scientific community is above 95% (or less than 5% chances of making an error). Inference framework Statistical inference uses very complex mathematical models to build a framework which will take the information from a sample of finite size (n) as an input and output estimate of parameters for the population of infinite size (along with degree of uncertainty). Then we can use these estimates of population parameters for the purpose of hypothesis testing or other interpretation downstream. Although, we are not interested in details of the mathematical models we must see what are the major components of this framework and how it process the information from a sample and what assumptions must be satisfied for the results from this framework to be valid. code here #population defined by mean and variance poplist &lt;- list(pop1=c(5, 0.04) , pop2= c(5.7, 0.0898), pop3 = c(6.2, 0.1599)) pop_col = c(\"blue\", \"darkgreen\", \"red\") plot(x=0,y=0,xlim= c(4,7.5), ylim=c(0,2), pch=NULL, ylab= \"density\", main=\"Distribution of 3 populations\", yaxt=\"n\") for(i in seq_along(poplist)){ curve(dnorm(x, mean=poplist[[i]][1], sd=sqrt(poplist[[i]][2])), col=pop_col[i], lwd=2,add=T) } #Standardized normal curve curve(dnorm(x), from = -4,to= +4, ylab= \"density\", xlab= \"z\", main= \"Standard normal distribution\", yaxt=\"n\", lwd=1.7) ##Sampling distribution of sample mean sample_size &lt;- c(5, 15, 25, 50) sample_colors &lt;- list(pop1= c(\"#89CFF0\",\"#6495ED\",\"#6082B6\",\"#5D3FD3\"), pop2= c(\"#AFE1AF\",\"#50C878\",\"#008000\",\"#355E3B\"), pop3= c(\"#F88379\",\"#C04000\",\"#E30B5C\",\"#A52A2A\")) plot(x=0,y=0,xlim= c(4,7.5), ylim=c(0,13), pch=NULL,xlab=\"sample mean\", ylab= \"density\", main=\"Sampling distribution of sample mean\", yaxt=\"n\") for(i in seq_along(poplist)){ for(sample in seq_along(sample_size)){ curve(dnorm(x, mean=poplist[[i]][1], sd=sqrt(poplist[[i]][2]/sample_size[sample])), col=sample_colors[[i]][sample],lwd=1.7, add=T) } } ###sampling distribution of sample variance # When sample size is close to 30 or more chi square distribution is similar to normal distribution plot(x=0,y=0,xlim= c(0,0.4), ylim=c(0,24), pch=NULL,xlab=\"sample variance\", ylab= \"density\", main=\"Sampling distribution of sample varaince \", yaxt=\"n\") for(i in seq_along(poplist)){ for(sample in 3:4){ curve(dnorm(x, mean=poplist[[i]][2], sd=sqrt((poplist[[i]][2])^2/(sample_size[sample]-1)*2)), col=sample_colors[[i]][sample],lwd=1.7, add=T) } } ####Plot chisquare for distribution plot(x=0,y=0,xlim= c(0,65), ylim=c(0,0.2), pch=NULL,xlab=\"Chi square\", ylab= \"density\", main=\"Chi-square distribution \", yaxt=\"n\") linecol &lt;- rainbow(4) size = c(5,15,25,50) for(i in 1:length(size)){ curve(dchisq(x, df=size[i]-1), from = 0,to= 65, col= linecol[i], add = T, lwd=1.7) } legend(30,0.2, legend = c( paste0(\"df = \", size-1 )), col= c(linecol), lty= c(1,1,1,1)) ###Plot t-distribution plot(x=0,y=0,xlim= c(-4,4), ylim=c(0,0.4), pch=NULL,xlab=\"t-statistic\", ylab= \"density\", main=\"t-distribution \", yaxt=\"n\") linecol &lt;- rainbow(4) size = c(5,15,25,50) for(i in 1:length(size)){ curve(dnorm(x), from = -4,to= 4, col= \"darkgreen\", ylab= \"density\", add=T, lwd=1.7) curve(dt(x, df=size[i]-1), from = -4,to= 4, col= linecol[i], add = T, lwd=1.7) } legend(2,0.4, legend = c(\"Std. Normal\", paste0(\"df = \", size-1 )), col= c(\"darkgreen\",linecol), lty= c(1,1,1,1,1)) ##### F-distribution plot(x=0,y=0,xlim= c(0,3), ylim=c(0,1.5), pch=NULL,xlab=\"F-statistic\", ylab= \"density\", main=\"F-distribution \", yaxt=\"n\") linecol &lt;- rainbow(4) size1 = c(5,15,25,50) size2 = c(10,23,4,50) for(i in 1:length(size)){ curve(df(x, df1=size1[i]-1, df2=size2[i]-1), col= linecol[i], add = T, lwd=1.7) } legend(2,1, legend = c( paste0(\"df1 = \", size1-1, \", df2 = \", size2-1 )), col= c(linecol), lty= c(1,1,1,1)) done! We need to acknowledge that population parameters (\\(\\mu\\)) and (\\(\\sigma^2\\)) are fixed and universal quantity and we will never know its true value (We will use the population parameters from our simulated data to illustrate the concepts only). The ultimate goal of inference is to get best estimate of these two parameters using only statistic calculated from a sample of size = n. The major components of the statistical framework are shown in figure 3. The population with mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)) is on the top of this framework and it is represented by distribution of random variable we are interested in (Example height of male aged between 20-39). The most important assumption about the population is that it follows normal distribution hence a standard normal distribution curve can be used to find probabilities related to the population. Two most important statistic we measure on a sample is sample mean \\((\\bar x)\\) and sample variance \\((s^2)\\). Central Limit Theorem gives the distribution of sampling distribution of sample mean which states that sample mean are also normally distributed and mean of such distribution is also population mean (\\(\\mu\\)), but has less variance than the population variance (figure 3) given by \\(\\frac{\\sigma^2}{n}\\). Therefore, larger the sample size gets less is the variance (shown by darker color for each population in figure 3). To get the probability associated with this distribution we can do a z-scale transformation for which we need population mean \\(\\mu\\) and population variance \\(\\sigma^2\\). \\[z= \\frac{\\bar Y- \\mu}{\\frac{\\sigma}{\\sqrt n}}\\] Hypothesis testing is designed in such a way that our question will have the population mean (some number we are comparing sample mean to, or 0 when we are comparing two populations) but we should be able to estimate the population variance using the sample variance. The top left plot in figure 3 show the distribution of sample variance. It is found that sample variance follows Chi-squared \\(\\chi ^2\\) distribution. Which shows, mean of the distribution of sample variance is population variance but unlike sample mean, when the sample size is small it is heavily skewed on the left and the shape of distribution depends on the degree of freedom. Which means sample variance calculated from a random sample of small size most probably gives the underestimate of the population variance. \\(\\chi ^2\\) distribution accounts for this discrepancy by providing separate distribution for samples of different size. To include this information in our z-score calculation a new statistic is calculated called t-statistic which follows t-distribution and similar to \\(\\chi^2\\) distribution it has different distribution for different sample size. Both \\(\\chi^2\\) and t-distribution have same shape as normal distribution when the sample size is greater than 30. Once we have this t-statistic we can use the probability given by t-distribution to find any probability associated with sampling distribution of sample means. \\[t-statistic = \\frac{\\bar Y- \\mu}{\\frac{s}{\\sqrt n}}\\] As you can see the all the variables in the equation can be calculated using a sample which enables us to compare the sample mean with the mean given in the question, when we consider a hypothetical population with \\(\\mu\\) = mean given by question and the sample we are working with is a sample from that population. The above method works best only when we need to compare two means. When we need to compare means from more than one population ANOVA is used for which inference framework has F-distribution which is basically ratio of two variance. If the variance is equal it has a value of 1 other wise it will be greater than 1. Details in ANOVA. Application Now that we have seen the basic components of inference, we can revisit our questions we are working on. ( NB: Please see notes focused on each distribution to learn more about them) Is the height of male (Age group 20-39 years old) in North America greater than 5.5 ? We could answer this question in much detail when we had the distribution of heights (defined by population mean and variance) in hand. But now we do not know population parameters hence inference is not able to provide answer to this question. This is one of the limitations of inferential statistics, rather than each observation we will be only focusing on the mean of populations, so we can still answer other questions below. Is the average height of male (Age group 20-39 years old) in North America greater than 5.5 ? Lets recall what we learned about hypothesis testing in the beginning. If we know the underlying distribution of a random variable we can check, how unusual a given value is to that distribution and if it is indeed very rare to observe that value in given distribution we can conclude, that value does not belong to the given distribution. In hypothesis testing we do exactly same but in little twisted way to be able to use the inference framework discussed above. In this setup we make a null hypothesis and it is exactly opposite to what we want in our question of interest. So why we need this null hypothesis? To answer this, lets see what is our question and what information we have about our population. Since we are not measuring every man living in N America, all we can do is measure heights in a randomly selected sample of size(n). Then, the mean we calculate from this sample(\\(\\bar x\\)) is just one value in the sampling distribution of the sample means for this population. We cannot calculate t-statistic with just sample mean and sample variance. We need the population mean. The question is giving us some clue. What you should visualize in your mind is that there are two populations in this question. The height of male (Age group 20-39 years old) in N America is the first and obvious population we have been seeing from the beginning, but the number we are tying to compare (5.5) is actually mean of a hypothetical population. Therefore, if we cannot calculate the population mean for our population, we can assume that our sample is taken from that hypothetical population (for which mean is given by the question) and check how unusual is this sample mean in that hypothetical population? If it comes out to be a rare event than we can reach to the conclusion that our sample does not belong to that hypothetical population. What we are doing by assuming our sample came from that hypothetical population given by question is building a null hypothesis. And at the end of the test we want to see the sample mean we get from our sample, to be a rare event for that hypothetical population. Which means we reject null hypothesis. To formally do this we first state the null hypothesis (\\(H_o\\)) and alternate hypothesis (\\(H_1\\)) \\[H_o: \\mu = 5.5\\] \\[H_1: \\mu &gt;5.5\\] Then calculate the t-statistic \\[t =\\frac{\\bar x - 5.5}{\\frac{s}{\\sqrt n}}\\] code for t-test ##Lets first make a sample of 100 from the population of NoA set.seed(45) set1 &lt;- sample(NorA, 100) cat(\"The t-statistic for this test is = \",(mean(set1)-5.5)/sqrt(var(set1)/100), \"\\n\") cat( \"The p-value for this test is = \",pt((mean(set1)-5.5)/sqrt(var(set1)/100), 99, lower.tail = FALSE), \"\\n \\n\") cat(\"##########--USING R t.test FUNCTION --########\\n\") t.test(set1, mu= 5.5, alternative = \"greater\") done! ## The t-statistic for this test is = 18.31421 ## The p-value for this test is = 7.324508e-34 ## ## ## ##########--USING R t.test FUNCTION --######## ## One Sample t-test ## ## data: set1 ## t = 18.314, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is greater than 5.5 ## 95 percent confidence interval: ## 6.147978 Inf ## sample estimates: ## mean of x ## 6.212581 code for fig below ##visualize ##BIG ASSUMPTION JUST FOR VISUALIZATION #for hypothetical population the variance is assumed to be equal to sample variance (just an approximation to see big picture ) plot(x=0,y=0,xlim= c(3,9), ylim=c(0,11), pch=NULL, ylab= \"density\", main=\"Distribution null hypothesis and alternative\", yaxt=\"n\") curve(dnorm(x, mean=6.2, sd= sqrt(var_NorA)), col=\"#f4c2c2\", lwd=2,add=T) curve(dnorm(x, mean=6.2, sd= sqrt(var_NorA/100)),n=500, col=\"red\", lwd=2,add=T) curve(dnorm(x, mean=5.5, sd= sqrt(var(set1))), col=\"#d2d1f9\", lwd=2,add=T) curve(dnorm(x, mean=5.5, sd= sqrt(var(set1)/100)), n=500,col=\"purple\", lwd=2,add=T) abline(v=5.5, col=\"#d2d1f9\" ) abline(v=6.2, col=\"#f4c2c2\", lwd=2 ) abline(v= mean(set1)) done! In the figure 4 we can clearly see, why p-value is so small. The distribution with green shade and the green dot at 6.21 are the only one distribution and data that is given to us by the question. The alternative hypothesis and other populations are shown using the data we collected from first section just to show big picture. What is the average height of male (Age group 20-39 years old) in North America, Europe, and Asia ? In absence of the knowledge of true population mean, the mean we get from a sample is the best estimate of the true population mean. But given the inherent variability in the sampling distribution of sample means there will be some level of uncertainty. To account for this instead of reporting the point estimate, we find a interval at certain significance level based of the sample mean. Lets start with North America, we will again randomly take a sample of size(n)= 50. And calculate the sample mean of this sample. Since we do not know the population mean, we will need to assume the population mean is sample mean in this sample and based off the distribution centered around this sample mean we will calculate the range of sample means which is encompass 95% of the sample means(exclude only 2.5% means on both extremes of distribution). Narrower this interval is better is the quality. The interpretation for CI at 95% significance, is 95% of the times we construct such intervals, true means will be within that interval. code for CI set.seed(100) set2 &lt;- sample(NorA, 50) mean_set2 &lt;- mean(set2) mean_set2 - (qt(0.025, 49, lower.tail = FALSE)*sqrt(var(set2)/50)) mean_set2 + (qt(0.025, 49, lower.tail = FALSE)*sqrt(var(set2)/50)) #Results can be confirmed by doing a test test for this mean t.test(set2, mu= mean_set2, alternative = \"two.sided\") done! ## Lower limit is 6.169956 ## Higher limit is 6.405479 ## ####---Using the function available in R ------##### ## One Sample t-test ## ## data: set2 ## t = 0, df = 49, p-value = 1 ## alternative hypothesis: true mean is not equal to 6.287718 ## 95 percent confidence interval: ## 6.169956 6.405479 ## sample estimates: ## mean of x ## 6.287718 code for plot ##visualize ##BIG ASSUMPTION JUST FOR VISUALIZATION #for unknow population with mean equal to sample mean the variance is assumed to be equal to sample variance (just an approximation to see big picture ) plot(x=0,y=0,xlim= c(5,7.3), ylim=c(0,11), pch=NULL, ylab= \"density\", main=\"Distribution null hypothesis and alternative\", yaxt=\"n\") polygon(c(seq(6, 6.5, 0.005),0), c(dnorm(seq(6, 6.5, 0.005), mean =mean_set2, sd=sqrt(var(set2)/100)),0), col=\"lightblue\") curve(dnorm(x, mean=6.2, sd= sqrt(var_NorA)), col=\"#f4c2c2\", lwd=2,add=T) curve(dnorm(x, mean=6.2, sd= sqrt(var_NorA/100)),n=500, col=\"red\", lwd=2,add=T) curve(dnorm(x, mean=mean_set2, sd= sqrt(var(set2))), col=\"#d2d1f9\", lwd=2,add=T) curve(dnorm(x, mean=mean_set2, sd= sqrt(var(set2)/100)), n=500,col=\"purple\", lwd=2,add=T) abline(v=mean_set2, col=\"#d2d1f9\" ) abline(v=6.2, col=\"#f4c2c2\", lwd=2 ) lines(c(6.1699, 6.4054), c(5,5)) done! The figure above show the hypothetical population around the sample mean from our sample (shaded blue color) and its relation to true sampling distribution of mean (red). Also at the bottom somewhat flat is the distribution of height. The black line is the 95% confidence interval and even for this sample we can see the true population mean lies within this interval. Is the average height of male (Age group 20-39 years old) in North America different than in Europe or Asia ? This is the question about comparing two populations of sample means. Therefore we need to polish our approach little bit. We now know that, for each populations we can build a sampling distribution of sample means, them mean of such distribution is the true mean of the population. When we need to compare two population we will have two such distributions of sampling means. As we have see so far, ultimately, we will need only one distribution and a value to see how uncommon it is in that distribution. Therefore, in case of comparing two populations we need to first find distribution for a random variable which is the difference between the sample means of two populations. The random variable \\(\\bar Y_1 - \\bar Y_2\\) also follows t-distribution and the t-statistic for large sample size is calculated by this formula: \\[t = \\frac{\\bar y1- \\bar y_2}{\\sqrt{\\frac {s_1^2}{n1}+\\frac{s_2^2}{n_2}}}\\] code for comparing means of two populations set.seed(500) ## is there difference between N. America and Asia cat(\"####----T-test between N America and Asia---------######\\n\") Sample_NAmerica &lt;- sample(NorA, 50) Sample_Asia &lt;- sample(Asia, 50) t.test(Sample_Asia, Sample_NAmerica, ) ## see for N. America and Europe cat(\"####----T-test between N America and Europe---------######\\n\") Sample_Euro &lt;- sample(Euro, 50) t.test(Sample_Euro, Sample_NAmerica) done! ## ####----T-test between N America and Asia---------###### ## ## Welch Two Sample t-test ## ## data: Sample_Asia and Sample_NAmerica ## t = -17.984, df = 69.395, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.306438 -1.045557 ## sample estimates: ## mean of x mean of y ## 5.006745 6.182742 ## ####----T-test between N America and Europe---------###### ## ## Welch Two Sample t-test ## ## data: Sample_Euro and Sample_NAmerica ## t = -6.0688, df = 92.631, p-value = 2.792e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.6059330 -0.3071451 ## sample estimates: ## mean of x mean of y ## 5.726203 6.182742 Conclusions: Using the methods of inference we are able estimate the population parameters with much more accuracy even with very small amount of data. This note provides only the overview of different hypothesis testing possible in statistics but hope this provided a clear concept of big picture of what is going on when we do a hypothesis testing."
  },{
    "title"    : "One-Way ANOVA: Learn by experimenting with data."
    ,"category" : "statistics"
    ,"tags"     : "anova, population difference, f test"
    ,"url"      : "/posts/2021-06-14-OneWay-ANOVA"
    ,"date"     : "Jan 12, 2021"
    ,"content"  : "Introduction   T-test is a popular method of comparing means between two populations but if we want to extend it beyond two populations there is serious decrease in power of test. On careful study of variance in multiple populations, early statisticians like Ronald Fisher found that analysis of variation can be used to check if there is difference in means of multiple populations which is popularly known as ANOVA in short. For example, in the figure below (fig 1) we can see distribution of test score in the population of four groups of students under two scenarios. First scenario in the left is when variance is small and second scenario on the right is when variance is large. Same group of students have same mean in both scenarios. Eyeballing these plots, we can say that the populations are more separated from each other in the first scenario than in the second. What does this separation means in the statistical terms is the difference in means in the populations in the left is more obvious than the one on the right. The more overlap is there among these populations less confident we are on the difference of their means. ANOVA provides the statistical framework of determining this confidence using just one sample from these populations. It compares if the variance between groups is significantly greater than the variance within the groups. R code for plot set.seed(3456) par(mfrow=c(1,2), mai= c(1,0,1,0)) #population defined by mean and variance #Scenario I poplist &lt;- list(pop1=c(22, 4) , pop2= c(25, 4), pop3 = c(30, 6), pop4= c(34,3)) pop_col = c(\"blue\", \"darkgreen\", \"red\", \"purple\") shade = c(rgb(0,0,255, max=255,alpha=100), rgb(0, 238, 0,max=255, alpha=100), rgb(173,0,0,max=255, alpha=100), rgb(160,32,240, max=255,alpha=100)) plot(x=0,y=0,xlim= c(10,48), ylim=c(0,0.25), pch=NULL, ylab= \"\", xlab=\"score\", main=\"Distribution when variance is low \\n means = (22, 25, 30, 34) \\n variance = (4, 4, 6, 3) \", yaxt=\"n\") for(i in seq_along(poplist)){ polygon(c(seq(10, 40, 0.5),10), c(dnorm(seq(10, 40, 0.5), mean =poplist[[i]][1], sd=sqrt(poplist[[i]][2])),0), col=shade[i]) curve(dnorm(x, mean=poplist[[i]][1], sd=sqrt(poplist[[i]][2])), col=pop_col[i], lwd=2,add=T) } #Scenario II poplist &lt;- list(pop1=c(22, 10) , pop2= c(25, 11), pop3 = c(30, 12), pop4= c(34,13)) pop_col = c(\"blue\", \"darkgreen\", \"red\", \"purple\") plot(x=0,y=0,xlim= c(10,48), ylim=c(0,0.25), pch=NULL, ylab=\"\",xlab=\"score\", main=\"Distribution when variance is high \\n means = (22, 25, 30, 34) \\n variance = (10, 11, 12, 13) \", yaxt=\"n\") for(i in seq_along(poplist)){ polygon(c(seq(10, 50, 0.5),10), c(dnorm(seq(10, 50, 0.5), mean =poplist[[i]][1], sd=sqrt(poplist[[i]][2])),0), col=shade[i]) curve(dnorm(x, mean=poplist[[i]][1], sd=sqrt(poplist[[i]][2])), col=pop_col[i], lwd=2,add=T) } legend(30,0.25, legend = c(\"Group A \", \"Group B\", \"Group C\", \"Group D\"), col= c(\"blue\", \"darkgreen\", \"red\", \"purple\"), lty= c(1,1,1,1), lwd=2) done! Fig 1: Distribution of population with different variance. Data collection We will use data from the same populations we discussed in the introduction. We will select a small sample of size(n)= 5 from each population in each scenario. Using R lets get such samples and see the distribution of the values in each sample using box plot (fig 2). code here set.seed(3456) par(mfrow=c(1,2)) ###Scenario I sampling levelA_1&lt;- round(rnorm(5, 22, 4),0) levelB_1 &lt;- round(rnorm(5, 25,4),0) levelC_1&lt;- round(rnorm(5,30,6),0) levelD_1&lt;- round(rnorm (5, 34, 3),0) levels &lt;- c(rep(\"A\",5),rep(\"B\",5),rep(\"C\",5),rep(\"D\",5)) boxplot(c(levelA_1,levelB_1,levelC_1, levelD_1)~levels, col=pop_col, ylim=c(6,60), main=\"Scenario I\", ylab=\"score\", border=\"azure4\") points(c(mean(levelA_1),mean(levelB_1), mean(levelC_1), mean(levelD_1) ), pch =18, col=1, cex=2) abline(h=mean(c(levelA_1,levelB_1,levelC_1, levelD_1)), col=\"blue\") stripchart(c(levelA_1,levelB_1,levelC_1, levelD_1)~levels, add =TRUE, vertical =TRUE, method= \"jitter\", col=\"darkgrey\", pch=19) ##Scenario II sampling levelA_2&lt;- round(rnorm(5, 22, 10),0) levelB_2 &lt;- round(rnorm(5, 25,11),0) levelC_2&lt;- round(rnorm(5,30,12),0) levelD_2&lt;- round(rnorm (5, 34, 13),0) levels &lt;- c(rep(\"A\",5),rep(\"B\",5),rep(\"C\",5),rep(\"D\",5)) boxplot(c(levelA_2,levelB_2,levelC_2, levelD_2)~levels, col=pop_col, ylim=c(6,60), main= \"Scenario II\", ylab=\"score\",border=\"azure4\") points(c(mean(levelA_2),mean(levelB_2), mean(levelC_2), mean(levelD_2) ), pch =18, col=1, cex=2) abline(h=mean(c(levelA_2,levelB_2,levelC_2, levelD_2)), col=\"blue\") stripchart(c(levelA_2,levelB_2,levelC_2, levelD_2)~levels, add =TRUE, vertical =TRUE, method= \"jitter\", col=\"darkgray\", pch=19) done! Fig 2: Box-plot of samples from two scenarios. In the box-plot above, the samples from the population shown in fig 1 are shown according to their group color in two different scenarios. The black diamond in each group is the group mean for that group and the blue horizontal line is the grand mean for all observations. Samples from two scenarios clearly reflect the difference in the variance in their population. Following the population variance, the observations in left are located close to the mean and in the right are more spread out. To get an idea about the difference in the means of these groups we calculated mean of each group (shown as black diamond). Group means in first scenario are 23.2, 25.0, 28.2, 33.2 in group A, B, C, and D respectively and in the second scenario it is 31.6, 24.2, 25.2, 38.8 in the group A, B, C, and D respectively. Now we need to determine if there is any difference in the mean of populations they are sampled from. Statement of the problem : Now that we have seen the data we will need to work with, lets define the question we are trying to solve. For each scenario, we saw some differences in the group means for these four samples from each group of students. But given these 4 samples (one from each group), do we have enough evidence to say the population means for these four groups are different ? Steps in ANOVA Key definitions We want to tackle this question by analysis of variance, ie we want to see, of the total variance in these scores how much of it comes from within the groups and how much of it comes from between the groups. 1.Total variance: It is the measure of total variance in the data (score). If we calculate the variance using the deviations of each observation from the grand mean it is called total variance and the sum of square used is called Total sum of squares (TSS). It has degree of freedom equal to n-1 just like normal sample variance. 2. Variance within: It is the measure of variance within each group. The deviation used is between each observation and its respective group mean. Sum of squares is called sum of square within (SSW). Since mean of each group was used as an estimate of population mean for that group it has n-t degree of freedom. See notes on degree of freedom for details about how to calculate df. Variance within is also referred as error as this is the part of total variance which could not be explained by our factor. 3. Variance between: It is the amount of variance explained by our factor. So, it is calculated using the deviations of group mean from the grand mean. It has degree of freedom equal to t-1 and sum of square is called sum of square between (SSB). This represents the variance explained by our factor. Figure below shows the relationships between these sum of squares in signal to noise analogy. TSS is proportional to the total variance in the random variable which is represented as the total background noise. When we introduce some categorical variable (groups), some of this variance in the random variable is explained by the groups, giving us some signal that this categorical variable has some effect on the random variable (represented as a pattern). If this signal takes higher proportion of TSS, then most of the variance can be attributed to the effect. For each groups, there is always some level of difference in the observations in the groups, which represents the amount of total variance that couldn’t be explained by our groups. The total sum of squares is always equal to sum of SSB and SSW. Fig 3: Understanding the partitioning of sum of squares in one-way ANOVA. Calculation of SS and mean SS. To illustrate the calculation of Sum of square we will look into the data from samples of Second scenario in more details. Figure below illustrates the calculation of SS. SSB is calculated using the black deviations in the above figure (deviation between grand mean and the group mean). In this example there are 5 samples in each level so the square of deviation needs to be multiplied by 5 before adding for each level. SSW is calculated group wise, using the deviation between each observation and the group mean (deviation represented by each group color). SST is just like regular variance using every observation in this data. So the deviation used will be between each observation and the grand mean. Now that we know how to calculate these sums lets calculate them for our samples using R click to view calculation in R # Scenario I Sum of squares grand_mean_I &lt;- mean(c(levelA_1,levelB_1,levelC_1, levelD_1)) group_mean_I &lt;- c(mean(levelA_1),mean(levelB_1), mean(levelC_1), mean(levelD_1) ) #TSS deviations &lt;- c(levelA_1, levelB_1, levelC_1, levelD_1)-grand_mean_I TSS &lt;- sum(sapply(deviations, function(x)x^2 )) cat(\"TSS = \", TSS, \"\\n\") #SSW deviations &lt;- group_mean_I - grand_mean_I SSB &lt;- sum((sapply(deviations, function(x)x^2))*5) cat(\"SSB = \", SSB, \"\\n\") #SSW deviations &lt;- c(levelA_1 - group_mean_I[1], levelB_1 - group_mean_I[2], levelC_1 - group_mean_I[3], levelD_1 - group_mean_I[4]) SSW &lt;- sum((sapply(deviations, function(x)x^2))) cat(\"SSW = \", SSW, \"\\n\") done! ## TSS = 594.8 ## SSB = 288.4 ## SSW = 306.4 F-test Once we have all sum of squares we can calculate the mean sum of squares by dividing the SS with degree of freedom. And since this mean sum of square is only the estimate of corresponding variance in the population we need to do formal hypothesis testing to see if it is statistically significant. Since we are comparing to variance we use F-test. \\[F-statistic = \\frac {\\frac{SSB}{t-1}}{\\frac {SSW}{n-t}}\\] \\[= \\frac{MSB}{MSW}\\] calculation in R ##Scenario I F-test F_statistic &lt;- (SSB/(4-1))/(SSW/(20-4)) F_statistic p_val &lt;- pf(F_statistic, 3, 16, lower.tail = F) p_val done! ## F-statistic = 5.020017 ## p-value = 0.01217514 We did it manually to illustrate the process. R has build in function to perform ANOVA. Lets use that build in function to perform ANOVA for the samples from both scenario. code here cat(\"#######---Scenario I----###########\\n\") summary(aov(c(levelA_1,levelB_1,levelC_1, levelD_1)~levels)) cat(\"\\n\\n########----Scenario II----#######\\n\") summary(aov(c(levelA_2,levelB_2,levelC_2, levelD_2)~levels)) done! ## #######---Scenario I----########### ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## levels 3 288.4 96.13 5.02 0.0122 * ## Residuals 16 306.4 19.15 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## ########----Scenario II----####### ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## levels 3 683.4 227.8 1.604 0.228 ## Residuals 16 2271.6 142.0 As we expected, because the samples in second scenario came from the populations where there was more variance and they did not separate well, our test came as insignificant. Only for the first scenario we can say there is difference between means of these groups of students (given the p-value of 0.012, there is less than 2% chance of error when we say so). If we want to say same statement for scenario II, there is 22.8% chance that we will be wrong. That is why test in second scenario is not significant. It is very important to note that, true mean of groups in our population is same in both scenarios and there is difference too. Because of the difference in variance in these scenarios, samples from first scenario has enough information to reflect that difference in population mean but in second scenario that difference in group means is lost in the noise added by high variance in that population. So our sample in second case doesn’t give us any conclusion, that is why insignificant results should not be interpreted is no different in means. It simply means the samples we are using do not have enough information to say there is difference in population means. In our situation, increasing the sample size to 30 will resolve the problem in second scenario as well. See code for details ###Scenario II large sample size summary(aov(c(round(rnorm(30, 22, 10),0) ,round(rnorm(30, 25,11),0), round(rnorm(30,30,12),0), round(rnorm (30, 34, 13),0))~ c(rep(\"A\",30),rep(\"B\",30),rep(\"C\",30),rep(\"D\",30)))) ## Df Sum Sq Mean Sq ## c(rep(\"A\", 30), rep(\"B\", 30), rep(\"C\", 30), rep(\"D\", 30)) 3 5001 1667.0 ## Residuals 116 18849 162.5 ## F value Pr(&gt;F) ## c(rep(\"A\", 30), rep(\"B\", 30), rep(\"C\", 30), rep(\"D\", 30)) 10.26 4.83e-06 *** ## Residuals ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 done! Will increasing the sample size always makes test significant ? Not really. If the samples are from the population with same mean, we cannot get significant result just by increasing the sample size. See code for details summary(aov(c(round(rnorm(300, 22, 10),0) ,round(rnorm(300, 22,11),0), round(rnorm(300,22,12),0), round(rnorm (300, 22, 13),0))~ c(rep(\"A\",300),rep(\"B\",300),rep(\"C\",300),rep(\"D\",300)))) ## Df Sum Sq ## c(rep(\"A\", 300), rep(\"B\", 300), rep(\"C\", 300), rep(\"D\", 300)) 3 917 ## Residuals 1196 159133 ## Mean Sq F value ## c(rep(\"A\", 300), rep(\"B\", 300), rep(\"C\", 300), rep(\"D\", 300)) 305.5 2.296 ## Residuals 133.1 ## Pr(&gt;F) ## c(rep(\"A\", 300), rep(\"B\", 300), rep(\"C\", 300), rep(\"D\", 300)) 0.0761 . ## Residuals ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 done! Conclusion One way ANOVA is very efficient tool to check if there is any effect of a factor we are interested. Effect of factor can be tested by performing ANOVA across different levels within that factor. The table below summarizes different sums of squares and degree of freedom used in one-way ANOVA. See my notes on Two-way ANOVA to learn more about ANOVA. Source df SS Mean SS F Between t-1 SSB MSB= SSB/t-1 F= MSB/MSW Within n-t SSW MSW= SSW/n-t   Total n-1 TSS    "
  },{
    "title"    : "Chi squared distribution"
    ,"category" : "statistics"
    ,"tags"     : "degree of freedom, expected value, sample variance, chi squared"
    ,"url"      : "/posts/2020-02-07-ChiSquare-Distribution"
    ,"date"     : "Feb 6, 2020"
    ,"content"  : "Distribution of sample variance Just like sampling distribution of sample mean, sample variance follows a distribution but it is slightly complex than, normal distribution followed sample mean. The shape of this distribution varies according the sample size. Therefore, there is a different curve to represent distribution of sample variance from samples of different sizes. Since, mean of distribution of sample variance (expected value) is equal to population variance, sample variance is also called unbiased estimate of population variance. The Mean and variance of such distribution is given by following formulas (keep reading for proof). \\[mean(s^2) = \\sigma^2\\] and variance of \\[Var(s^2)= \\frac{2(\\sigma^2)^2}{v}\\] Where, \\(v\\) is degree of freedom given by n-1. To get some intuition, lets make some simulated distribution of sample variance from a population with mean 70 and variance 100 taking samples of size(n) = {3,5,9,15,50,100}, re-sampled 1000 times, Figure 1 shows the histogram of sample variance of those 1000 samples for each size. code here par(mfrow=c(3,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(3,5,9,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) sample_variance&lt;-c (sample_variance, var(sample)) } hist(sample_variance,breaks = 200, main=paste0('Dist of sample variance n = ', sample_size[size],' \\n mean of var = ' ,round(mean(sample_variance),1), \" variance of var = \", round(var(sample_variance),1)), xlim=c(0,400), ylim = c(0,60)) } done! As you can see from the figure above the shape of histogram is different for each sample size and mean and variance of these distribution are also very close to the one that could be calculated by above formulas. When the sample size is small (n=3), it is positively skewed with long tail on right and most of the mass of distribution at zero. What this suggesting is when we use only 3 observations, most of the time we get very small variance. But how does it compare to the population variance? With such small sample size although mean of the whole distribution is still equal to the population variance, there is high probability that a randomly selected sample variance will underestimate the true variance. As we increase the sample size to 5, the mass of distribution moves slightly right and is slightly less skewed than n=3. As we keep increasing our sample size to 100, the distribution looks much like normal distribution with both mean and median equal to population variance and there is much less variance in sample variance compared to our smallest sample size (n=3). This signifies that if we have big enough sample size, a randomly selected sample is much likely to represent the population variance. \\(\\chi^2\\) distribution We know variance is mean of sum of square of deviations. If we look at only the deviations, it is, a random variable that follows a normal distribution (because both \\(X\\) and \\(\\bar X\\) follows normal distribution). Since, all other terms in variance are constant, we can find the distribution of sample variance if we know the distribution of sum of square of deviations. And this is exactly what \\(\\chi^2\\) gives. To simplify the calculations it is always calculated from the standard normal distribution and is applicable for any random variable that follows standard normal distribution. Suppose \\(Z\\) is a random variable following standard normal distribution \\(Z \\sim N(0,1)\\). Then the distribution given by the sum of square of n items from this distribution follows \\(\\chi ^2\\) distribution which is defined by single parameter, the number of independent items used in the calculation of sum of \\(Z^2\\) (\\(v\\)). \\[\\chi^2= \\sum Z^2\\] \\(\\chi^2\\) distribution has following properties: Being the sum of squares, it can never be less than zero. The shape of distribution changes with the number of z used to calculation the sum. The mean of the distribution is given by \\(v\\) and variance given by \\(2v\\). When the sample size is larger (&gt;30), the \\(\\chi^2\\) distribution is very similar to normal distribution. It is so similar that a normal distribution with mean \\(z=\\frac{X-v}{\\sqrt(2v)}\\) can be used calculate z and used std normal distribution. To build the intuition, lets see it in R, first by making a standard normal distribution and then sampling \\(z\\) of different sample size and then calculating the sum of square of \\(z\\)s in that sample. Histograms shows \\(\\chi^2\\) distribution at degree of freedom equal to {3,5,9,15,100}. See if our histograms comply with the properties of \\(\\chi^2\\) distribution. code here par(mfrow=c(3,2)) set.seed(56) stdNormal &lt;- rnorm(1000) sample_size = c(3,5,9,15,50,100) for (size in 1:length(sample_size)) { sumXsq &lt;- NULL for (i in 1:1000) { X &lt;- sample(stdNormal, sample_size[size]) sumXsq&lt;-c (sumXsq, sum(X^2)) } hist(sumXsq,breaks = 200, main=paste0('Dist of Sum of X squared n = ', sample_size[size],' \\n mean = ' ,round(mean(sumXsq),1), \" variance = \", round(var(sumXsq),1)), xlim=c(0,150), ylim = c(0,30)) } done! Also visualize how these distributions will actually look when produced from the mathematical formula for \\(\\chi^2\\) distribution. code here curve(dnorm(x), from = -3,to= 20, col= \"darkgreen\", ylab= \"density\", xlab=\"Chi Squared\") linecol &lt;- rainbow(6) size = c(3,5,9,15,50,100) for(i in 1:length(size)){ curve(dchisq(x, df=size[i]-1), from = 0,to= 20, col= linecol[i], add = T) } legend(13,0.4, legend = c(\"Std Normal\", paste0(\"df = \", size )), col= c(\"darkgreen\",linecol), lty= c(1,1,1,1,1,1,1)) done! \\(\\chi^2\\) statistic Now the we understand the \\(\\chi^2\\) distribution, lets see how can we establish relationship between sample variance and this distribution. From the equation for \\(\\chi^2\\) we know : \\[\\chi^2= \\sum Z^2\\] \\[= \\sum\\left(\\frac{Y-\\bar Y}{\\sigma}\\right)^2\\] \\(= \\frac{\\sum(Y- \\bar Y)^2}{\\sigma^2}\\) We know the equation for sample variance is \\(s^2= \\frac{\\sum(Y- \\bar Y)^2}{n-1}\\), Substituting the value of sum of squares (SS); \\[\\chi^2= \\frac{(n-1)s^2}{\\sigma^2}\\] One we have this \\(\\chi^2\\) statistics we can get the idea about the probability of variance of particular sample from \\(\\chi^2\\) distribution which follows n-1 degree of freedom. Check my notes on degree of freedom if you are not sure why we need to use n-1 degree of freedom. Lets also see how \\(\\chi^2\\) statistics would distribute in our simulate data. code here par(mfrow=c(3,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(3,5,9,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) sample_variance&lt;-c (sample_variance, var(sample)) chisq &lt;- (length(sample)-1)*sample_variance/100 } hist(chisq,breaks = 200, main=paste0('Dist of chisq n = ', sample_size[size],' \\n mean of chisq = ' ,round(mean(chisq),1), \" variance of chisq = \", round(var(chisq),1)), xlim=c(0,150), ylim = c(0,30)) } done! Derivation of mean of sampling distribution of variance This distribution is exactly same as we got from standard normal distribution in figure above. We can also use this equation and the properties of \\(\\chi^2\\) to derive the mean and variance of distribution of sample variance. From the equation for \\(\\chi^2\\) \\(\\chi^2= \\frac{vs^2}{\\sigma^2}\\) \\(\\text{or, } s^2 = \\frac{\\chi^2 \\sigma^2}{v}\\) From the property of \\(\\chi^2\\) distribution, the value of \\(\\chi^2\\) at mean of the distribution is \\(v\\). \\[mean(s^2)=\\frac{v\\sigma^2}{v}\\] \\[mean(s^2)= \\sigma^2\\] This again, shows expected value of sampling distribution of sample variance is equal to population variance. Variance of sampling distribution of variance To prove this we can start from the fact that we know variance of \\(\\chi^2\\) distribution is \\(2v\\) \\[Var(\\chi^2) = 2v\\] \\[Var\\left(\\frac{vs^2}{\\sigma^2}\\right)=2v\\] \\[\\frac{v^2}{\\sigma^4}Var(s^2)= 2v\\] \\[Var(s^2)=\\frac{2\\sigma^4}{v}\\] Conclusion Although we do not use \\(\\chi^2\\) most often directly in hypothesis testing, but this distribution is the one which establish a connecting link between population variance (\\(\\sigma^2\\)) and sample variance (\\(s^2\\)). Therefore any hypothesis testing where population variance is estimated based on the sample variance this distribution is being used under the hood which we will see more clearly in the t-distribution and F-distribution."
  },{
    "title"    : "Expected value"
    ,"category" : "statistics"
    ,"tags"     : "random variable, expected value, sampling distribution"
    ,"url"      : "/posts/2019-11-01-Expected-value"
    ,"date"     : "Oct 31, 2019"
    ,"content"  : "Expected value Expected value of a random variable Frequently called Expected value, expectation, expectancy, mathematical expectation, sometime referred simply as mean, average or first moment. For a random variable X it is found to be represented by any one of these symbols \\(\\text{ E(X), E[X], EX, E, \\mathbb{E} }\\). Definition Once we know the probability distribution of a random variable we can find the expected value of that random variable, which is exactly what the name is suggesting, it is average or mean value of the random variable which we expect to find most often. Lets plot binomial probability distribution of two random variables (X and Y) with same number of trials but different p.  code here par(mfrow=c(1,2)) random_var &lt;- 0:10 probability &lt;- NULL for (i in 1:11) { k=random_var[i] #probability[i] &lt;- choose(10, k)*(0.2^k)*(0.8)^(10-k) probability[i]&lt;- dbinom(k, size=10, prob = 0.2) } barplot( probability, names.arg = random_var, main = \"B(X~10,0.2)\", xlab= \"random variable, X\", ylab= \"probability\") #Next binomial probability random_var &lt;- 0:10 probability &lt;- NULL for (i in 1:11) { k=random_var[i] #probability[i] &lt;- choose(10, k)*(0.2^k)*(0.8)^(10-k) probability[i]&lt;- dbinom(k, size=10, prob = 0.7) } barplot( probability, names.arg = random_var, main = \"B(Y~10,0.7)\", xlab= \"random variable, Y\", ylab= \"probability\") done! So for the plot we can see that X, is most often expected be 2 and Y is most often expected to be 7. Therefore, \\[E[X] = 2\\] and \\[E[Y] = 7\\] Finite probability distribution Now lets see how to it is calculated mathematically. For a random variable, X with finite probability distribution the expected value of X, E[X] is equal to the weighted mean of all possible outcomes (note that random variables are always numeric values). For example in binomial distribution \\((X \\sim B(n,p))\\) we have only n+1 number of possible outcomes (k) each with probability of P(X=k) defined by following equation: \\[P(Y=k)= {n \\choose k}p^k(1-p)^{n-k}\\] Therefore, when we calculate the weighted average of all outcomes \\(k\\), (weighted by their probability) then it is the expected value of random variable X. \\[E[X] = \\sum_{k=0}^n k.{n \\choose k}p^k(1-p)^{n-k}\\] On further solving the equation this simplifies to \\[= np\\] For continuous density function Random variables like sample mean which follow normal distribution which is a continuous probability density function the expected value needs to be calculated by using integration but concept can be understood as similar to above discussed for binomial distribution. Wikipedia has elaborate explanation about expected value of different probability distribution wiki Check the table half way in this page."
  },{
    "title"    : "All you need to know about degree of freedom in statistics"
    ,"category" : "statistics"
    ,"tags"     : "degree of freedom, expected value, sample variance, bias"
    ,"url"      : "/posts/2019-07-01-Degree-of-freedom"
    ,"date"     : "Jun 30, 2019"
    ,"content"  : "Background Degree of freedom (df) is a very important mathematical concept which is implemented in multiple disciplines like mechanics, physics, chemistry, and also in inferential statistics. In its essence, it is the number of independent variables or parameters of a system. Or it is the number of independent quantities necessary to express the values of all the variable properties of a system. For example a point moving in a 3D space has degree of freedom equal to 3 because three coordinate values are necessary to define the state of that point at any given time. However, if we put a constrain on the system and fix one of the axis at a constant, then the point is free to move along only two axis and has only 2 degree of freedom. Df in statistics Df is extensively applied in inferential statistics where we are trying to estimate some population parameters (mean, variance, skewness, and kurtosis) using the measurement made in a random sample from that population (sample statistics). For any estimator of population parameter, degree of freedom is equal to number of independent pieces of information that go into the estimate of that parameter or alternatively it is number of variables in a statistic minus the number of estimated parameters used while computing that statistic. These definitions may look vague but lets see how two most commonly used statistics: sample mean (\\(\\bar x\\)) and sample variance(\\(s^2\\)) works and come back to these definitions. Goal of inferential statistics One of the main goal of statistics is to estimate the population parameters like population mean (\\(\\mu\\)) and population variance (\\(\\sigma^2\\)). Theoretically, it may be possible to make measurement on population but practically, the population always have infinite number of items so we are always forced to take a sample of size (n). For many other practical reasons, this sample size is usually very small which makes the situation even worse (we will see how). So we need to be able to estimate population parameters by using a small sample and that estimate needs to unbiased. Bias is a very commonly used term in statistics and in addition to its meaning in normal English language it has some statistical value. A random variable is considered to be an unbiased estimator of a population parameter only if the expected value of the random variable is equal to the population parameter. (See note on expected value here). Similarly a random variable is an biased estimate of a population parameter if the expected value of the random variable is not equal to the population parameter. It is called overestimate if the expected value is greater than the population parameter and vice-versa. Mathematically, an estimator gives an unbiased estimate of population parameter if, \\[E[estimator] = \\text {population parameter}\\] Sample mean Now lets come back to sample mean. If X represents a random variable that measure sampling distribution of sample means, (see note on distribution of sampling means) according to the Central limit theorem it is an unbiased estimate of population mean.ie: \\[E[X] = \\mu\\] proof here We can prove this with some manipulation of \\(E[]\\) Substitute X with the formula for sample mean \\[E[X] = E\\left[\\frac{1}{n}\\sum_{i=1}^nx_i\\right]\\] \\[=\\frac{1}{n}\\sum_{i=1}^n E[x_i]\\] Expectation of a single observation is equal to population mean \\(\\mu\\). \\[=\\frac{1}{n}\\sum_{i=1}^n \\mu\\] \\[= \\frac{1}{n}n\\mu\\] \\[=\\mu\\] done! Now, lets lets figure out the degree of freedom for the sample mean. This is the equation for calculating a sample mean: \\[\\bar x = \\frac{1}{n}\\sum_{i=1}^nx_i\\] For calculating the value of each sample mean, only the values coming from each observation are used. If \\(n\\) is the sample size, then all \\(n\\) values are used and no any other intermediate estimates are computed. Therefore, the degree of freedom by our definition above is \\(n-0\\), or in simple form \\(n\\). So, for the sample mean, the degree of freedom is same as sample size. This seems relatively straight forward, lets see sample variance next which will further clarify it. Sample variance The generally used formula for calculating a sample variance is given by or derived from this equation : \\[\\begin{equation} \\tag{equation 1} s^2=\\frac {\\sum (x- \\bar x)^2}{n-1} \\end{equation}\\] By definition, variance is mean of squared deviation, so if we don’t think much and just go by definition, the equation for sample variance should be: \\[\\begin{equation} \\tag{equation 2} s^2_{bi}=\\frac {\\sum (x- \\bar x)^2}{n} \\end{equation}\\] This seems very plausible, but unlike sample mean, in calculation of sample variance we need one intermediate estimate for population parameter (\\(\\mu\\)) given by sample mean \\(\\bar x\\). Although, sample mean is an unbiased estimate of population mean, the mean calculated from only one sample is subject to inherent randomness, and because of this randomness the sample variance calculated using sample mean according to equation 2 is a biased estimate of population variance. It will almost always underestimate the population variance (report less than what actually is). The amount of bias in the sample variance can be mathematically shown to be equal to \\(\\frac{n}{n-1}\\) (see below for proof). Therefore, to get an unbiased estimate of population variance we need to multiply equation 2 by this factor. proof here Proof for biased sample mean Lets us write equation 2 into computational easy form. See my note on mean and variance if you are not familiar with derivation of computational form of variance. \\[s^2_{bi}=\\frac {\\sum x^2}{n}-\\left(\\frac {\\sum x}{n}\\right)^2\\] And also write the symbol of s as random variable measuring sample variance \\(S^2_{bi}\\) and calculate expected value of that random variable. \\[E[S^2_{bi}]=E\\left[\\frac {\\sum x^2}{n}-\\left(\\frac {\\sum x}{n}\\right)^2 \\right]\\] \\[=E\\left[\\frac {\\sum x^2}{n}\\right]-E\\left[\\left(\\frac {\\sum x}{n}\\right)^2 \\right]\\] The term \\(\\frac{\\sum x}{n}\\) is the sample mean \\(\\bar x\\) \\[=E\\left[\\frac {\\sum x^2}{n}\\right]-E\\left[\\left(\\bar x\\right)^2 \\right]\\] Lets process \\(E\\) further in \\[\\begin{equation} \\tag{equation 3} =\\frac {\\sum E[x^2]}{n}-E\\left[\\bar x^2\\right] \\end{equation}\\] For any given random variable Y we know, \\[Var(y) = E[y^2]-\\left(E[y]\\right)^2\\] \\[\\text{or, } E[y^2] = Var(y) + \\left(E[y]\\right)^2\\] In equation 3 there are two terms which evaluates to expectation of square, so expanding them equation 3 becomes \\[E[S^2_{bi}] = \\frac{\\sum Var(x)+(E[x])^2}{n}- Var(\\bar x) - (E[\\bar x])^2\\] Now, lets simplify some of these terms, expectation of a variable is its mean so, \\(E[x] = \\mu\\). From, central limit theorem, variance of distribution of sample mean is \\(\\frac{\\sigma ^2}{n}\\) and sample mean being the unbiased estimate of population mean \\(E[\\bar x] = \\mu\\). Now, replace these values in the equation above, \\[E[S^2_{bi}] = \\frac{\\sum \\sigma^2+(\\mu)^2}{n}- \\frac{\\sigma ^2}{n} - (\\mu)^2\\] Solving this: \\[= \\frac{( \\sigma^2+(\\mu)^2)n}{n}- \\frac{\\sigma ^2}{n} - (\\mu)^2\\] \\[= \\sigma ^2 + \\mu ^2- \\frac{\\sigma ^2}{n}- \\mu ^2\\] \\[\\begin{equation} \\tag{equation 4} E[S^2_{bi}] = \\sigma ^2 \\left(\\frac{n-1}{n}\\right) \\end{equation}\\] Therefore, the expected value of biased sample variance is always less than the population variance by a factor of (n-1)/n. For large sample size this factor tends to be equal to 1 but on a small sample size it is profound. done! \\[\\text{Unbiased }s^2= s^2 = s^2_{bi}. \\frac{n}{n-1}\\] \\[=\\frac {\\sum (x- \\bar x)^2}{n}.\\frac{n}{n-1}\\] Removing n, it gives back the equation 1. \\[s^2 =\\frac {\\sum (x- \\bar x)^2}{n-1}\\] Therefore, the equation we have been using for sample variance is actually, equation 2 (given by the definition of variance) but adjusted for bias associated with uncertainty in estimate of one the population parameter during its calculation. And because we used this one intermediate population estimate (sample mean, \\(\\bar x\\)), we need to subtract 1 from the number of values used to calculate sample variance. Thus, sample variance has n-1 degree of freedom. This is very important for a small size sample, because as n gets close to 1, subtracting 1 will decrease denominator by large value increasing the expected value of random variable measuring sample variance. This increase will balance the variance underestimated by biased equation (equation 2). The plot below show the distribution of sample variance (adjusted) with different sample size n= {5,15,50,100}. The mean reported is the expected value of each distribution. Regardless of the sample size all four plots have expected value almost equal to the population variance (100). Figure 1: Distribution of sample variance. code for Fig: 1 par(mfrow=c(2,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(5,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) sample_variance&lt;-c (sample_variance, var(sample)) } hist(sample_variance, main=paste0('Dist of sample variance n = ', sample_size[size],' \\n mean of var = ' ,round(mean(sample_variance),1), \" variance of var = \", round(var(sample_variance),1)), xlim=c(0,400)) } done! On the other hand, the plots below are reporting the biased measure of sample variance (given by equation 2 above) for exactly same experiment reported above. The expected value is seriously lower than the population variance and the situation is worse when sample size is lowest. This under estimation occurs because with small sample size, sample mean lies within that sample, all the deviations becomes smaller than they actually need to be. With small sample size there is less chance that true mean will be within or near that sample. Figure 2: Distribution of biased variance code for Fig:2 par(mfrow=c(2,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(5,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) #biased variance sample_variance&lt;-c (sample_variance, var(sample)*(sample_size[size]-1)/sample_size[size]) } hist(sample_variance, main=paste0('Biased sample variance n = ', sample_size[size],' \\n mean of var = ' ,round(mean(sample_variance),1), \" variance of var = \", round(var(sample_variance),1)), xlim=c(0,400)) } done! Quick recap Lets summarize what we learned from sample mean and sample variance so far: degree of freedom for a random variable estimating a population parameter is equal to number of independent information that went into calculation of that estimate minus the number of intermediate estimated population parameters used in calculation of such an estimate. All the estimates of population parameters should be an unbiased estimate of that population parameter. However, if degree of freedom is less than the number variables used in calculation of that estimate, it is always biased. ie \\(\\text {df = n, --------&gt; estimate is unbiased estimate of population parameter (Eg: sample mean)}\\) \\(\\text {df &lt; n, --------&gt; estimate is biased estimate of population parameter (Eg: sample variance)}\\) This bias in the estimate of a population variance \\(\\sigma ^2\\) using a sample variance can be adjusted by dividing the sum of squares (SS) by degree of freedom rather than taking the literal mean of squared deviations. All these three statements above are universally applicable to any situation where we need to estimate mean of squared deviations. Therefore it is applicable to all the mean SS (MSB, MSW, Mean TSS, MSR, MSE) calculated during ANOVA and linear regression. One way ANOVA The source of variations in one way ANOVA are between levels of factors, within each level and total variation. The table below shows how df is calculated for each of the sources: Source variables intermediate pop estimates df Total all observations(n) mean of y (1) n-1 Between number of levels (t) mean of y (1) t-1 Within all observations (n) mean of each levels(t) n-t Total variance calculation is same as the variance estimate we saw above. But when it comes to the SSB, deviation used is the one between mean of corresponding level to the estimate of population mean of y. So, we have used one intermediate estimate. And within each level for a balanced experiment, there are n/t number of observations, and n/t number of deviations corresponding to each observation, but all the deviations within a level is essentially using only one independent piece of information (mean of that level). So, total number of independent pieces of information is equal to the number of levels. And therefore, df is t-1. For within variation, all observations contribute their share of information but since deviation is taken from the mean of the corresponding level that observation belonged to. Here level of each mean is used as the estimate of true mean of that level not as a variable, as in SSB. Therefore, df for within variance is n-t. Two way ANOVA For a two way ANOVA with two factors A and C each with a and c number of levels and n observation in each cell. Source variables intermediate pop estimates df Total all observations(acn-1) mean of y (1) acn-1 Between cells no. of cells (ac) mean of y (1) ac-1 Factor A no. of levels in A (a) mean of y (1) a-1 Factor C no. of levels in C (c) mean of y (1) c-1 A*C ac, a, c mean of y (3 times) (a-1)(c-1) Within cells all observations (acn) mean of each cell(ac) ac(n-1) All the other SS has similar idea as one way ANOVA except interaction term. Sum of square for AC is defined as: \\(SS_{AC} = SS_{Cells}-SS_A-SS_C\\) It is the variance explained only by cells. It is calculated as \\[SS_{AC} = n\\sum_1^{ac}[(mean_{cell} - \\bar y_{total}) - (mean_{A} - \\bar y_{total}) - (mean_{C} - \\bar y_{total})]^2\\] The first deviation is from mean of a particular cell to estimate of mean of y. The second deviation is from mean of particular level of A to the estimate of mean of y. Since there are only a number of levels in A, those each \\(mean_A\\) will be reused c times. The third deviation is from mean of a particular level of C to the estimate of mean of y. It is clear that there is only one intermediate estimate of population parameter \\(\\bar y\\). But this it is used three times for calculating three different types of deviations. Each type of deviation has its own df and total df is calculated by as df for cell minus df for A minus df for C. \\[df(AC) = df(cell)- df(A) - df(C)\\] \\[= ac -1 - a + 1 - c +1\\] \\[(a-1)(c-1)\\] Simple linear regression \\[y = \\beta_0 + \\beta_1x + \\epsilon\\] Source variables intermediate pop estimates df Total all observations(n) mean of y (1) n-1 Regression number of reggressor +1 mean of y (1) 1+1-1=1 Error all observations (n) number of regressor +1 n-2 This can be interpreted in very similar way to one way anova. Other application So far we saw profound use of df to get unbiased estimate of any type/partition of variance. In addition to this df is used to adjust the standard normal distribution when the population parameters are unknown and only sample statistics are used to calculate the test statistics. The test statistics \\(z=\\frac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt n}}\\) follows standard normal distribution with mean 0 and variance 1. But in may real case situation both \\(\\mu\\) and \\(\\sigma\\) are unknown. So, standard normal distribution is not appropriate to calculate p-value. The distribution needs to be adjusted for the added uncertanity because of using sample variance. Therefore a new test statistics has been deviced called t-statistic. \\[t=\\frac{\\bar x - \\mu}{\\frac{s}{\\sqrt n}}\\] and the only parameter defining this distribution is degree of freedom \\(v\\)associated with sample variance. This distribution has mean of 0 for \\(v\\) &gt;1, otherwise undefined. and variance = \\(\\frac{v}{v-2}\\) for \\(v\\) &gt; 2. Unlike standard normal distribution, there is new curve for each value of \\(v\\). T-distribution always has fatter tails because of the uncertainty associated with sample variance used in calculation of the t-statistics. Figure 3: PDF curve for Normal and t-distribution. code for fig: 3 curve(dnorm(x), from = -4,to= 4, col= \"red\", ylab= \"density\") curve(dt(x, df=5), from = -4,to= 4, col= \"blue\", add = TRUE) curve(dt(x, df=30), from = -4,to= 4, col= \"green\", add = TRUE) legend(2,0.3, legend = c(\"Std. normal \", \"t, df= 5\", \"t, df= 30\"), col= c(\"red\", \"blue\", \"green\"), lty= c(1,1,1)) done! The \\(\\chi^2\\) and \\(F\\) distribution also has probability density function adjusted for the degree of freedom. To build the intuition I highly suggested to watch this video by Justin."
  },{
    "title"    : "Mean and Variance; covering the base"
    ,"category" : "statistics"
    ,"tags"     : "mean, variance"
    ,"url"      : "/posts/2018-12-02-Mean-N-Variance"
    ,"date"     : "Dec 1, 2018"
    ,"content"  : "Basic concept It is very important to understand the measure of central tendency, mean and measure of dispersion, variance in statistics.General formula for mean is: \\[\\begin{equation} \\tag{eq 1} Mean (\\mu) = \\frac{\\sum x}{N} \\end{equation}\\] When frequency table is given mean is: \\[\\begin{equation} \\tag{eq 2} Mean (\\mu) = \\frac{\\sum fx}{N} \\end{equation}\\] When relative frequency (empirical probability/proportion) is given, mean is given by: \\[\\begin{equation} \\tag{eq 3} Mean (\\mu) = \\sum ( x\\times p) \\end{equation}\\] where p is the proportion of particular random variable X. In probability, the expected value of a random variable, E[x] is similar to mean of that random variable. And is given as: \\[\\begin{equation} \\tag{eq 4} E[X] = \\sum x \\times P(X=x) \\end{equation}\\] where: E[X] = Expectation of random variable X P(X=x) = Probability that random variable has value of x. If all values of random variable are equally likely, than P(X=x) is equal to 1/N and since 1/N is constant can be moved outside summation which makes this Equation equal to the general equation for mean. More in binomial distribution. Measure of dispersion Dispersion is the extent of divergence from the center of data. Standard deviation (\\(\\sigma\\)) and variance (\\(\\sigma ^2\\)) are the two measures of dispersion. To understand why these two measures exist and what do they actually explain about dispersion, lets first see how a dispersion of a variable can be studied. To understand the extent of divergence is a three step process: First need to determine what is the center of data (we use mean \\(\\mu\\) for this) Second measure the extent of divergence of each observation from that center. Finally, summarize these divergences by taking the average of all divergence. ie \\(\\frac{\\sum divergence }{N}\\) The first and third step are relatively straight forward but we need to look at the second step further. The simplest form of divergence we can measure is deviation from mean ie (\\(x-\\mu\\)). But because of the nature of mean, when we attempt to sum all the divergence, it is always equal to 0 which is not helpful. Alternatively, to neutralize the effect of negative deviation, we can square the deviation for each value of a random variable and calculate the average of these squared deviation. For this deviation from mean is first squared and added to get the sum of square (SS). \\[\\begin{equation} \\tag{eq 5} SS=\\sum (x- \\mu)^2 \\end{equation}\\] One natural question to raise here is why we are using squared deviation? Taking absolute deviation would also solve the problem of 0 deviation in total. Squared deviations is not a random choice but very carefully thought by early statisticians because squared deviations and thus variance has some very useful mathematical properties when solving calculus and derivative problems. Once we have SS it can be divided by N to get the mean of squared deviation which is called variance. Although variance is best for mathematical prospective, one problem with variance is its unit. It has square unit compared to the random variable we are studying. For example if we are studying weight in kg, variance has unit of \\(kg^2\\), which is difficult to interpret the dispersion. Therefore standard deviation \\(\\sigma\\) is frequently reported which is equal to square root of variance. \\[\\begin{equation} \\tag{eq 6} \\sigma^2=\\frac {\\sum (x- \\mu)^2}{N} \\end{equation}\\] The sum of squares (SS) and can be solved further to get simplified form. \\[SS=\\sum (x- \\mu)^2\\] \\[= \\sum x^2 -2x\\mu + \\mu^2\\] Now summation can be processed into each term \\[= \\sum x^2 -2\\mu\\sum x + \\mu^2\\sum1\\] Now replace the value of \\(\\mu = \\frac {\\sum x}{N}\\) \\[= \\sum x^2 -2\\frac {\\sum x}{N}\\sum x + \\left(\\frac {\\sum x}{N}\\right)^2.N\\] Solving this we get \\[\\begin{equation} \\tag{eq 7} \\text{Sum of squares (SS)}=\\sum x^2 - \\frac {(\\sum x)^2}{N} \\end{equation}\\] This equation 7 is used for calculating SS most often than equation 6 because it is computationally very easy to calculate both for human and computers. For a population, variance is then obtained by dividing sum of square of deviation by N. \\[\\begin{equation} \\tag{eq 8} Variance = \\sigma^2 = \\frac {\\sum x^2}{N}-\\left(\\frac {\\sum x}{N}\\right)^2 \\end{equation}\\] This equation of variance can be further explained in terms of probability. In this equation first term is mean of \\(x^2\\) and second term is square of mean of x. It is very important to identify the difference between these two terms. As we know mean is also called as expectation of a random variable in probability. \\[Variance = E[X^2]-(E[X])^2\\] \\[\\text{or, } Variance= \\sum x^2.P(X=x^2)- \\left(\\sum x.P(X=x)\\right)^2\\] Sample variance Usually variance is calculated from a sample of observation so, while calculating the SS, deviations need to be calculated from sample mean and sum of square of deviation is divided by n-1 to get an unbiased estimate of population variance (See details in degree of freedom). \\[\\begin{equation} \\tag{eq 9} \\text{Sample variance} = s^2 = \\frac {\\sum x^2 - \\frac {\\left(\\sum x\\right)^2}{n}}{n-1} \\end{equation}\\]"
  },{
    "title"    : "Brief note on Permutation and Combination"
    ,"category" : "statistics"
    ,"tags"     : "permutation, combination"
    ,"url"      : "/posts/2018-06-22-Permutation-n-Combination"
    ,"date"     : "Jun 21, 2018"
    ,"content"  : "Permutation and combinations are a part of core counting problem in mathematics. In biology also we encounter multiple situations where knowledge of permutations and combinations are handy. Here is a brief introduction to these concepts with the focus on biological problems. 1. Permutation    This is sampling problem where the position of items is significant. For example: a DNA sequence of certain length like ATT CGA AAT is a set of 9 nucleotide from a set of four: A T C G. So, how many possible DNA sequences of length 9 are possible? Lets review some concepts: 1.1 Permutation where repetition is allowed: This is the most straight forward situation to understand. Lets say we need to get a subset of 3 items (k) from a nucleotide set of containing 4 items (n) . \\(Nucleotide = \\{A,T,C,G\\}\\). Then, each position has n possible options. \\[\\text{Permutation with repetition} = \\frac{n}{}.\\frac{n}{}.\\frac{n}{}....k\\] This can be written more concisely as: \\[\\text{Permutation with repetition} = n^k\\] Where: n = total number of items in a set k = number of items in the subset (this can be greater than n as well) Now, coming back to the question about DNA sequence of length 9, there are \\(9^4=6561\\) possible DNA sequences. 1.2 Permutations without repetition In many situations, like how many ways 10 athletes can be arranged in 1st, 2nd and 3rd position. Same athlete can not come in both 1st or other positions. So in this situation, 1st position has n options but second has n-1 and third has n-2 possibilities. \\(\\text{Permutation without repetition} = \\frac{n}{}.\\frac{n-1}{}.\\frac{n-2}{}....\\frac{n-k}{}\\) Mathematically, this is written using factorial \\[\\text{Permutation without repetition} = \\frac{n!}{(n-k)!}\\] It is important to note that subset here cannot be greater than the set itself. Therefore, in the example of athletes, there are \\(\\frac{10!}{(10-3)!}=720\\) 2. Combination This is a sampling problem where the position is not significant. We can sample k items from a set of n items with repetition or without repetition. 2.1 Combination without repetition This can be taken as a special case of permutation without repetition, where subset with same items irrespective of their position is same. For example {1,2,3}, {1,3,2}, {2,1,3} are all same as {1,2,3}. Therefore we need to adjust the formula for permutation as follows: \\(\\text{Combination without repetition} = \\frac{n!}{(k!)(n-k)!}\\) In R this can be calculated using choose function. #Find possible combinations of 3 items from set of 5 choose(5,3) ## [1] 10 #To see all possible combinations combn(1:5, 3) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 1 1 1 1 1 2 2 2 3 ## [2,] 2 2 2 3 3 4 3 3 4 4 ## [3,] 3 4 5 4 5 5 4 5 5 5 In R to get the number of permutation, we can multiply the result of choose function with \\(k!\\). #find possible permutation without repetition in R when 3 selected from 5 choose(5,3)*factorial(3) ## [1] 60 # when repetition is allowed 5^3 ## [1] 125 Application of combination One of the most popular application of combination is in binomial equation. \\[P(Y=k)= {n \\choose k}p^k(1-p)^{n-k}\\] In this equation \\(n \\choose k\\) represents the possible ways by which k success can occur when the trial is repeated n times. Another application is when we need to calculate the number of pairwise list of genes from a set of genes. If there are 10 genes are we need to calculate the pairwise correlation between all possible pairs then it is given by : \\(_{10}C_{2}= {10 \\choose 2}\\) which is equal to 45. Fig: Pairwise combination of 10 items. Lets look at this pairwise combination in more detail. There are \\(10^2=100\\) permutation possible with repetition, which represents all 100 squares in figure above. The combination which gives 45 represents only one half of the diagonal (either upper or lower). This number (45) is also equal to the number of possible paths in a non-directed network with 10 nodes. And if the network is directed then permutation without repetition can be used which will give 90 possible paths in that network, which is the number of all squares except diagonal."
  },{
    "title"    : "Notebook"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/projects.html"
    ,"date"     : "Aug 1, 2022"
    ,"content"  : "main header Project Notebooks info My projects are organized into following notebooks. text_color white img :heading.jpg back_color lightblue category title Statistics type id_statistics color gray title R-programming type id_R color #62b462 title Python type id_python color #2FD0ED title Bioinformatics type id_bioinformatics color purple list type id_statistics project_name Statistics - The intuitive way project_excerpt The impressive and complicated models frequently showcased as triumph of cutting edge advancement in AI and computer science on the heart relies on the principles and theorems developed in statistics and mathematics. Therefore one of my top priority projects is to have clear idea on concepts like sampling distribution hypothesis testing analysis of variance and experimental design. img :Statistics.svg img_title img statistics1 date 2020-03-13 post # Statistics !Troy Data Science meme(:Stat2.jpg):data-align=\\ center\\ &ensp;&ensp;One of the struggles I had while learning statistic was to build intuitive thinking about fundamental concepts of statistics like inference hypothesis testing and probability distribution. This is where I adopted the **learn by experimenting** method. It is really hard to build intuition just by seeing one or two datasets presented in a textbook. Being a student of science I wanted to experiment every statement and theories discussed in a textbook. The best tool I found to do so is **R** which is basically a metaverse for statistics. Every theories in statistics can be tested in R by generating some dummy data. It has support for any type of distribution and we can simulate any kind of sample. Doing so helped me skip theoretical proof of theorems in statistics but still get clear picture of what the equation is doing. I can generate thousands of samples in R and see the distribution of sample means and variance which will be very close to the one given by the mathematical equation. This approach helped me a lot to learn both statistics and R. So I want to share my notes with everyone who wants to learn statistics with minimum mathematics. All my notes on category of `Statistics` are my attempts to explain each topics with as much experiment as possible. 1. Principle of inference(../posts/2022-01-13-Principle-of-inference) 2. Mean and Variance(../posts/2018-12-02-Mean-N-Variance) 3. Permutation and combination(../posts/2018-06-22-Permutation-n-Combination) 4. Probability theory 5. Statistical distribution 5.1 Binomial distribution 5.2 Normal Distribution (t-distribution $$\\\\chi^2$$(../posts/2020-02-07-ChiSquare-Distribution) F- distribution) 6. Hypothesis testing(../posts/2022-01-13-Principle-of-inference) 7. ANOVA (One-way and Factorial)(../posts/2021-06-14-OneWay-ANOVA) 8. Regression (Simple multiple)(../posts/2022-03-03-Simple-linear-regression) 9. Degree of freedom(../posts/2019-07-01-Degree-of-freedom) type id_R project_name Visualization in R project_excerpt Re-visualize some famous plots using ggplot2 img :R.svg img_title img Rstudio date 2020-04-23 post # GGplot2 for visualization Data visualization can be done by using multiple tools like excel ggplot js or other. GGplot is one of the most flexible and powerful tool. type id_python project_name Learning Python for R users project_excerpt Tidy way to learn python img :python.svg img_title img python date 2021-05-27 post # Background Different people start learning to code at different time point in there career with different goals. I myself had started coding because ... type id_bioinformatics project_name Bioinformatics projects project_excerpt DGCA MEGENA and rna seq img :bioinformatics.svg img_title img bioinformatics date 2021-06-08 post These are the there projects I am working on now. 1. DGCA(../posts/2022-07-01-html_insidemd) 2. MEGENA 3. EDGER"
  },{
    "title"    : "Links"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/links.html"
    ,"date"     : "Aug 1, 2022"
    ,"content"  : "main header Some useful learning sources info This is the collection of some of my favourite resources my first and last resort. I am grateful to all the contributors in these websites category title Bioinformatics type id_bioinformatics color #800020 title Data Visualization type id_dataViz color #3F6104 title Programming type id_programming color #62b462 title Statistics type id_Stat color #0080aa title JekyII / Liquid type id_jekyiiliquid color gray title Web Design type id_webdesign color #F4A273 list type id_Stat title Random Services url https://www.randomservices.org/random/dist/index.html info Excellent resource for in-depth understanding of statistics type id_Stat title ML with Andrew Ng url https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN info Full course on ML provided by Andrew Ng from Stanford University. 112 videos covering topics on ML and AI. type id_dataViz title Dataviz Inspiration url https://www.dataviz-inspiration.com/ info Visualize hundreds of methods by which data can be visualized. type id_dataViz title ggplot2 url https://ggplot2-book.org/coord.html info ggplot2: Elegant Graphics for Data Analysis By Hadley Wickham Danielle Navarro and Thomas Lin Pedersen. type id_dataViz title Data visualization url https://clauswilke.com/dataviz/index.html info Fundamentals of Data Visualization: A primer on making informative and compelling figures - By Claus O. Wilke . type id_programming title Advanced R url https://adv-r.hadley.nz/ info Advanced R: Second edition by Hadley Wickham type id_jekyiiliquid title Jekyll url https://jekyllrb.com/ info Transform your plain text into static websites and blogs. type id_jekyiiliquid title Jekyll Cheat Sheet url https://cloudcannon.com/community/jekyll-cheat-sheet/ info There are so many Jekyll variables and filters to remember and it can be tricky to keep it all in your head. This cheat sheet serves as a quick reference of everything Jekyll can do. type id_jekyiiliquid title Liquid for Designers url https://github.com/Shopify/liquid/wiki/Liquid-for-Designers info Liquid for Designers wiki on GitHub type id_jekyiiliquid title Liquid for Programmers url https://github.com/Shopify/liquid/wiki/Liquid-for-Programmers info Liquid for Programmers wiki on GitHub type id_jekyiiliquid title Liquid Reference url https://shopify.dev/api/liquid/ info Liquid is a template language created by Shopify and written in Ruby. It is now available as an open source project on GitHub type id_webdesign title W3Schools url https://www.w3schools.com/ info W3Schools offers free online tutorials references and exercises in all the major languages of the web. Covering popular subjects like HTML CSS JavaScript Python SQL Java and many more. type id_bioinformatics title Harvard course url https://liulab-dfci.github.io/bioinfo-combio/ info Collection of tutorials and videos from multiple instructors."
  },{
    "title"    : "Amrit Koirala"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/"
    ,"date"     : "Aug 1, 2022"
    ,"content"  : "  Data science has become a fad now-a-days and has gained a lot of attention from new generation. Because of rapid switch from the analogous to the digital world, every data is stored in an electronic form now. Every tweet we share, photos uploaded to social media, transactions made online, records of temperature at every corner of the world, stock exchange data, and demographic data from census are all stored online or offline in some form of electronic media in the form of 0s and 1s. This huge collection of data is both the mother and the operand of data science. The sole goal of data science is to be able to manage this huge collection of data (big data), recognize the patterns which might explain the question of interest and visualize the patterns to effectively communicate the observation. This huge collection of data is both the mother and the operand of data science. Source : NCBI stat   Biology and human health science have also made a significant dive into the digital data pool. As shown by the NCBI Gene Bank data above, sequencing data size has sky rocketed in last two decades. In addition to improvement in sequencing technology, spectrometric and imaging techniques have also been polished adding huge amount of data of various kinds like images, videos and 3D structures of proteins. To understand these big data in biology it is essential to make the optimum use of knowledge from diverse filed like statistics, mathematics and computer science. This web site is the online record of my journey to understand and analyze biological data using tools available in various programming languages like R, Python, and bash. Here you will find notes on various topics on Statistics, R, Python, bash scripting and miscellaneous topics on latest biological and bioinformatics techniques. Hope you will enjoy coming back here. ✍"
  },{
    "title"    : "About"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/about.html"
    ,"date"     : "Aug 1, 2022"
    ,"content"  : "“The only true wisdom is in knowing you know nothing.”                                 - Socrates   I am a bioinformatician by profession working at Baylor College of Medicine, Houston, TX. During my PhD research at SDSU, I realized the enormity of sequencing data and the potential knowledge that could be mined from the publicly available databases like Genebank and Uniprot. This ignited my interest into the bioinformatics and eventually I completed master’s in data science to get the firm grasp of cutting-edge techniques in bigdata management and machine learning. I have an extensive experience working with genomic data, phylogenetic analysis, gene expression data and other omics. R and python are my favorite computer languages and I like learning and teaching new statistical methods specialized for the study of multiple -omics data in biology. Area of expertise   Data science being an inter-disciplinary subject needs expertise in range of fields. I would like to delineate my area of interest and expertise burrowing the concept of data science venn diagram by Drew Conway. Education South Dakota State University, SD, USA Doctor of Philosophy - PhD, Microbiology, General 2016 - 2021 Master’s Degree, Data Science. Programme ranked second best by Intelligent. 2018 - 2020 Tribhuvan University, Kathmandu, Nepal Masters of Science, Medical Microbiology 2012 -2015 Bachelor’s Degree in Science, Microbiology 2009 - 2012 Publications google scholar Koirala, Amrit. 2021. “Free-Living Diazotrophs and the Nitrogen Cycle in Natural Grassland Revealed by Culture Dependent and Independent Approaches.” PhD thesis, South Dakota State University. Agrahari, Gaurav, Amrit Koirala, Roshan Thapa, Mahesh Kumar Chaudhary, and Reshma Tuladhar. 2019. “Antimicrobial Resistance Patterns and Plasmid Profiles of Methicillin Resistant Staphylococcus Aureus Isolated from Clinical Samples.” Nepal Journal of Biotechnology 7 (1): 8–14. Koirala, Amrit, Gaurav Agrahari, Nabaraj Dahal, Prakash Ghimire, and KR Rijal. 2017. “ESBL and MBL Mediated Resistance in Clinical Isolates of Nonfermentating Gram Negative Bacilli (NFGNB) in Nepal.” J Microb Antimicrob Agents 3 (1): 18–24. Koirala, Amrit, and Volker S Brözel. 2021. “Phylogeny of Nitrogenase Structural and Assembly Components Reveals New Insights into the Origin and Distribution of Nitrogen Fixation Across Bacteria and Archaea.” Microorganisms 9 (8): 1662. Tamang, Kamal, Pashupati Shrestha, Amrit Koirala, Jagat Khadka, Narayan Gautam, and Komal Raj Rijal. 2018. “Prevalence of Bacterial Uropathogens Among Diabetic Patients Attending Padma Nursing Hospital of Western Nepal.” Amrit koirala"
  }
  ]

