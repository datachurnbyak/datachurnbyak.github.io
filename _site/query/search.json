[
    {
    "title"    : "All you need to know about degree of freedom in statistics"
    ,"category" : "statistics"
    ,"tags"     : "degree of freedom, expected value, sample variance, bias"
    ,"url"      : "/posts/2019-07-01-Degree-of-freedom"
    ,"date"     : "Jun 30, 2019"
    ,"content"  : "Background Degree of freedom (df) is a very important mathematical concept which is implemented in multiple disciplines like mechanics, physics, chemistry, and also in inferential statistics. In its essence, it is the number of independent variables or parameters of a system. Or it is the number of independent quantities necessary to express the values of all the variable properties of a system. For example a point moving in a 3D space has degree of freedom equal to 3 because three coordinate values are necessary to define the state of that point at any given time. However, if we put a constrain on the system and fix one of the axis at a constant, then the point is free to move along only two axis and has only 2 degree of freedom. Df in statistics Df is extensively applied in inferential statistics where we are trying to estimate some population parameters (mean, variance, skewness, and kurtosis) using the measurement made in a random sample from that population (sample statistics). For any estimator of population parameter, degree of freedom is equal to number of independent pieces of information that go into the estimate of that parameter or alternatively it is number of variables in a statistic minus the number of estimated parameters used while computing that statistic. These definitions may look vague but lets see how two most commonly used statistics: sample mean (\\(\\bar x\\)) and sample variance(\\(s^2\\)) works and come back to these definitions. Goal of inferential statistics One of the main goal of statistics is to estimate the population parameters like population mean (\\(\\mu\\)) and population variance (\\(\\sigma^2\\)). Theoretically, it may be possible to make measurement on population but practically, the population always have infinite number of items so we are always forced to take a sample of size (n). For many other practical reasons, this sample size is usually very small which makes the situation even worse (we will see how). So we need to be able to estimate population parameters by using a small sample and that estimate needs to unbiased. Bias is a very commonly used term in statistics and in addition to its meaning in normal English language it has some statistical value. A random variable is considered to be an unbiased estimator of a population parameter only if the expected value of the random variable is equal to the population parameter. (See note on expected value here). Similarly a random variable is an biased estimate of a population parameter if the expected value of the random variable is not equal to the population parameter. It is called overestimate if the expected value is greater than the population parameter and vice-versa. Mathematically, an estimator gives an unbiased estimate of population parameter if, \\[E[estimator] = \\text {population parameter}\\] Sample mean Now lets come back to sample mean. If X represents a random variable that measure sampling distribution of sample means, (see note on distribution of sampling means) according to the Central limit theorem it is an unbiased estimate of population mean.ie: \\[E[X] = \\mu\\] proof here We can prove this with some manipulation of \\(E[]\\) Substitute X with the formula for sample mean \\[E[X] = E\\left[\\frac{1}{n}\\sum_{i=1}^nx_i\\right]\\] \\[=\\frac{1}{n}\\sum_{i=1}^n E[x_i]\\] Expectation of a single observation is equal to population mean \\(\\mu\\). \\[=\\frac{1}{n}\\sum_{i=1}^n \\mu\\] \\[= \\frac{1}{n}n\\mu\\] \\[=\\mu\\] done! Now, lets lets figure out the degree of freedom for the sample mean. This is the equation for calculating a sample mean: \\[\\bar x = \\frac{1}{n}\\sum_{i=1}^nx_i\\] For calculating the value of each sample mean, only the values coming from each observation are used. If \\(n\\) is the sample size, then all \\(n\\) values are used and no any other intermediate estimates are computed. Therefore, the degree of freedom by our definition above is \\(n-0\\), or in simple form \\(n\\). So, for the sample mean, the degree of freedom is same as sample size. This seems relatively straight forward, lets see sample variance next which will further clarify it. Sample variance The generally used formula for calculating a sample variance is given by or derived from this equation : \\[\\begin{equation} \\tag{equation 1} s^2=\\frac {\\sum (x- \\bar x)^2}{n-1} \\end{equation}\\] By definition, variance is mean of squared deviation, so if we don’t think much and just go by definition, the equation for sample variance should be: \\[\\begin{equation} \\tag{equation 2} s^2_{bi}=\\frac {\\sum (x- \\bar x)^2}{n} \\end{equation}\\] This seems very plausible, but unlike sample mean, in calculation of sample variance we need one intermediate estimate for population parameter (\\(\\mu\\)) given by sample mean \\(\\bar x\\). Although, sample mean is an unbiased estimate of population mean, the mean calculated from only one sample is subject to inherent randomness, and because of this randomness the sample variance calculated using sample mean according to equation 2 is a biased estimate of population variance. It will almost always underestimate the population variance (report less than what actually is). The amount of bias in the sample variance can be mathematically shown to be equal to \\(\\frac{n}{n-1}\\) (see below for proof). Therefore, to get an unbiased estimate of population variance we need to multiply equation 2 by this factor. proof here Proof for biased sample mean Lets us write equation 2 into computational easy form. See my note on mean and variance if you are not familiar with derivation of computational form of variance. \\[s^2_{bi}=\\frac {\\sum x^2}{n}-\\left(\\frac {\\sum x}{n}\\right)^2\\] And also write the symbol of s as random variable measuring sample variance \\(S^2_{bi}\\) and calculate expected value of that random variable. \\[E[S^2_{bi}]=E\\left[\\frac {\\sum x^2}{n}-\\left(\\frac {\\sum x}{n}\\right)^2 \\right]\\] \\[=E\\left[\\frac {\\sum x^2}{n}\\right]-E\\left[\\left(\\frac {\\sum x}{n}\\right)^2 \\right]\\] The term \\(\\frac{\\sum x}{n}\\) is the sample mean \\(\\bar x\\) \\[=E\\left[\\frac {\\sum x^2}{n}\\right]-E\\left[\\left(\\bar x\\right)^2 \\right]\\] Lets process \\(E\\) further in \\[\\begin{equation} \\tag{equation 3} =\\frac {\\sum E[x^2]}{n}-E\\left[\\bar x^2\\right] \\end{equation}\\] For any given random variable Y we know, \\[Var(y) = E[y^2]-\\left(E[y]\\right)^2\\] \\[\\text{or, } E[y^2] = Var(y) + \\left(E[y]\\right)^2\\] In equation 3 there are two terms which evaluates to expectation of square, so expanding them equation 3 becomes \\[E[S^2_{bi}] = \\frac{\\sum Var(x)+(E[x])^2}{n}- Var(\\bar x) - (E[\\bar x])^2\\] Now, lets simplify some of these terms, expectation of a variable is its mean so, \\(E[x] = \\mu\\). From, central limit theorem, variance of distribution of sample mean is \\(\\frac{\\sigma ^2}{n}\\) and sample mean being the unbiased estimate of population mean \\(E[\\bar x] = \\mu\\). Now, replace these values in the equation above, \\[E[S^2_{bi}] = \\frac{\\sum \\sigma^2+(\\mu)^2}{n}- \\frac{\\sigma ^2}{n} - (\\mu)^2\\] Solving this: \\[= \\frac{( \\sigma^2+(\\mu)^2)n}{n}- \\frac{\\sigma ^2}{n} - (\\mu)^2\\] \\[= \\sigma ^2 + \\mu ^2- \\frac{\\sigma ^2}{n}- \\mu ^2\\] \\[\\begin{equation} \\tag{equation 4} E[S^2_{bi}] = \\sigma ^2 \\left(\\frac{n-1}{n}\\right) \\end{equation}\\] Therefore, the expected value of biased sample variance is always less than the population variance by a factor of (n-1)/n. For large sample size this factor tends to be equal to 1 but on a small sample size it is profound. done! \\[\\text{Unbiased }s^2= s^2 = s^2_{bi}. \\frac{n}{n-1}\\] \\[=\\frac {\\sum (x- \\bar x)^2}{n}.\\frac{n}{n-1}\\] Removing n, it gives back the equation 1. \\[s^2 =\\frac {\\sum (x- \\bar x)^2}{n-1}\\] Therefore, the equation we have been using for sample variance is actually, equation 2 (given by the definition of variance) but adjusted for bias associated with uncertainty in estimate of one the population parameter during its calculation. And because we used this one intermediate population estimate (sample mean, \\(\\bar x\\)), we need to subtract 1 from the number of values used to calculate sample variance. Thus, sample variance has n-1 degree of freedom. This is very important for a small size sample, because as n gets close to 1, subtracting 1 will decrease denominator by large value increasing the expected value of random variable measuring sample variance. This increase will balance the variance underestimated by biased equation (equation 2). The plot below show the distribution of sample variance (adjusted) with different sample size n= {5,15,50,100}. The mean reported is the expected value of each distribution. Regardless of the sample size all four plots have expected value almost equal to the population variance (100). Figure 1: Distribution of sample variance. code for Fig: 1 par(mfrow=c(2,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(5,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) sample_variance&lt;-c (sample_variance, var(sample)) } hist(sample_variance, main=paste0('Dist of sample variance n = ', sample_size[size],' \\n mean of var = ' ,round(mean(sample_variance),1), \" variance of var = \", round(var(sample_variance),1)), xlim=c(0,400)) } done! On the other hand, the plots below are reporting the biased measure of sample variance (given by equation 2 above) for exactly same experiment reported above. The expected value is seriously lower than the population variance and the situation is worse when sample size is lowest. This under estimation occurs because with small sample size, sample mean lies within that sample, all the deviations becomes smaller than they actually need to be. With small sample size there is less chance that true mean will be within or near that sample. Figure 2: Distribution of biased variance code for Fig:2 par(mfrow=c(2,2)) set.seed(56) #make a dummy population with N= 1000, \\mu = 70 and \\sigma= 10 pop&lt;- rnorm(1000, mean= 70, 10) sample_size = c(5,15,50,100) for (size in 1:length(sample_size)) { sample_variance &lt;- NULL for (i in 1:1000) { sample &lt;- sample(pop, sample_size[size]) #biased variance sample_variance&lt;-c (sample_variance, var(sample)*(sample_size[size]-1)/sample_size[size]) } hist(sample_variance, main=paste0('Biased sample variance n = ', sample_size[size],' \\n mean of var = ' ,round(mean(sample_variance),1), \" variance of var = \", round(var(sample_variance),1)), xlim=c(0,400)) } done! Quick recap Lets summarize what we learned from sample mean and sample variance so far: degree of freedom for a random variable estimating a population parameter is equal to number of independent information that went into calculation of that estimate minus the number of intermediate estimated population parameters used in calculation of such an estimate. All the estimates of population parameters should be an unbiased estimate of that population parameter. However, if degree of freedom is less than the number variables used in calculation of that estimate, it is always biased. ie \\(\\text {df = n, --------&gt; estimate is unbiased estimate of population parameter (Eg: sample mean)}\\) \\(\\text {df &lt; n, --------&gt; estimate is biased estimate of population parameter (Eg: sample variance)}\\) This bias in the estimate of a population variance \\(\\sigma ^2\\) using a sample variance can be adjusted by dividing the sum of squares (SS) by degree of freedom rather than taking the literal mean of squared deviations. All these three statements above are universally applicable to any situation where we need to estimate mean of squared deviations. Therefore it is applicable to all the mean SS (MSB, MSW, Mean TSS, MSR, MSE) calculated during ANOVA and linear regression. One way ANOVA The source of variations in one way ANOVA are between levels of factors, within each level and total variation. The table below shows how df is calculated for each of the sources: Source variables intermediate pop estimates df Total all observations(n) mean of y (1) n-1 Between number of levels (t) mean of y (1) t-1 Within all observations (n) mean of each levels(t) n-t Total variance calculation is same as the variance estimate we saw above. But when it comes to the SSB, deviation used is the one between mean of corresponding level to the estimate of population mean of y. So, we have used one intermediate estimate. And within each level for a balanced experiment, there are n/t number of observations, and n/t number of deviations corresponding to each observation, but all the deviations within a level is essentially using only one independent piece of information (mean of that level). So, total number of independent pieces of information is equal to the number of levels. And therefore, df is t-1. For within variation, all observations contribute their share of information but since deviation is taken from the mean of the corresponding level that observation belonged to. Here level of each mean is used as the estimate of true mean of that level not as a variable, as in SSB. Therefore, df for within variance is n-t. Two way ANOVA For a two way ANOVA with two factors A and C each with a and c number of levels and n observation in each cell. Source variables intermediate pop estimates df Total all observations(acn-1) mean of y (1) acn-1 Between cells no. of cells (ac) mean of y (1) ac-1 Factor A no. of levels in A (a) mean of y (1) a-1 Factor C no. of levels in C (c) mean of y (1) c-1 A*C ac, a, c mean of y (3 times) (a-1)(c-1) Within cells all observations (acn) mean of each cell(ac) ac(n-1) All the other SS has similar idea as one way ANOVA except interaction term. Sum of square for AC is defined as: \\(SS_{AC} = SS_{Cells}-SS_A-SS_C\\) It is the variance explained only by cells. It is calculated as \\[SS_{AC} = n\\sum_1^{ac}[(mean_{cell} - \\bar y_{total}) - (mean_{A} - \\bar y_{total}) - (mean_{C} - \\bar y_{total})]^2\\] The first deviation is from mean of a particular cell to estimate of mean of y. The second deviation is from mean of particular level of A to the estimate of mean of y. Since there are only a number of levels in A, those each \\(mean_A\\) will be reused c times. The third deviation is from mean of a particular level of C to the estimate of mean of y. It is clear that there is only one intermediate estimate of population parameter \\(\\bar y\\). But this it is used three times for calculating three different types of deviations. Each type of deviation has its own df and total df is calculated by as df for cell minus df for A minus df for C. \\[df(AC) = df(cell)- df(A) - df(C)\\] \\[= ac -1 - a + 1 - c +1\\] \\[(a-1)(c-1)\\] Simple linear regression \\[y = \\beta_0 + \\beta_1x + \\epsilon\\] Source variables intermediate pop estimates df Total all observations(n) mean of y (1) n-1 Regression number of reggressor +1 mean of y (1) 1+1-1=1 Error all observations (n) number of regressor +1 n-2 This can be interpreted in very similar way to one way anova. Other application So far we saw profound use of df to get unbiased estimate of any type/partition of variance. In addition to this df is used to adjust the standard normal distribution when the population parameters are unknown and only sample statistics are used to calculate the test statistics. The test statistics \\(z=\\frac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt n}}\\) follows standard normal distribution with mean 0 and variance 1. But in may real case situation both \\(\\mu\\) and \\(\\sigma\\) are unknown. So, standard normal distribution is not appropriate to calculate p-value. The distribution needs to be adjusted for the added uncertanity because of using sample variance. Therefore a new test statistics has been deviced called t-statistic. \\[t=\\frac{\\bar x - \\mu}{\\frac{s}{\\sqrt n}}\\] and the only parameter defining this distribution is degree of freedom \\(v\\)associated with sample variance. This distribution has mean of 0 for \\(v\\) &gt;1, otherwise undefined. and variance = \\(\\frac{v}{v-2}\\) for \\(v\\) &gt; 2. Unlike standard normal distribution, there is new curve for each value of \\(v\\). T-distribution always has fatter tails because of the uncertainty associated with sample variance used in calculation of the t-statistics. Figure 3: PDF curve for Normal and t-distribution. code for fig: 3 curve(dnorm(x), from = -4,to= 4, col= \"red\", ylab= \"density\") curve(dt(x, df=5), from = -4,to= 4, col= \"blue\", add = TRUE) curve(dt(x, df=30), from = -4,to= 4, col= \"green\", add = TRUE) legend(2,0.3, legend = c(\"Std. normal \", \"t, df= 5\", \"t, df= 30\"), col= c(\"red\", \"blue\", \"green\"), lty= c(1,1,1)) done! The \\(\\chi^2\\) and \\(F\\) distribution also has probability density function adjusted for the degree of freedom. To build the intuition I highly suggested to watch this video by Justin."
  },{
    "title"    : "Projects"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/projects.html"
    ,"date"     : "Jul 18, 2022"
    ,"content"  : "main header Projects info Some projects I am working on text_color white img :heading.jpg back_color lightblue category title Statistics type id_statistics color gray title R-programming type id_R color #62b462 title Python type id_python color #2FD0ED title Bioinformatics type id_bioinformatics color purple list type id_statistics project_name Mathematics and Statistics project_excerpt The impressive and complicated models frequently showcased as triumph of cutting edge advancement in AI and computer science on the heart relies on the principles and theorems developed in statistics and mathematics. Therefore one of my top priority projects is to have clear idea on concepts like sampling distribution hypothesis testing analysis of variance and experimental design. img :Statistics.svg img_title img statistics1 date 2020-03-13 post # Statistics !Troy Data Science meme(:Stat2.jpg):data-align=\\ center\\ &ensp;&ensp;Statistics is a fascinating discipline to study and as you start to understand the essence of techniques used in statistics and develop some intuitive thinking about data and probability it becomes quite engaging and rewarding as well. Having its origin deeply rooted in the field of mathematics and probability theory its becomes quite intimidating for anybody from non-mathematics background to dive deep into the theories used in statistics. The general tendency among learners with non-mathematics background is to skip any apparently complex topic as unnecessary mathematical detail. By doing so one may learn to use the statistical tools but when its time to interpret the results or when one needs to design their own experiment from scratch its hard to do so because of lack of crystal clear understanding of statics. I struggled myself with this a lot during my undergraduate and early gradate years. It is really hard to know how deep one needs to go so that you can grasp the clear idea about statistics but you do not need dig back your notes on algebra and calculus from highschool classes. &ensp;&ensp;One of the struggles I had while learning statistic was to build intuitive thinking about fundamental concepts of statistics like population sample population parameters and sample statistics. This is where I developed my own method of learning statistics which is **learn by experimenting**. It is really hard to build intuition just by seeing one or two datasets presented in a textbook. Being a student of science I couldn't swallow every formula without completely understanding it. I wanted to experiment every statement and theories discussed in a textbook. The best tool I found to do so is **R** which is basically a metaverse for statistics. Every theories in statistics can be tested in R by generating some dummy data. It has support for any type of distribution and we can simulate any kind of sample. Doing so helped me skip theoretical proof of theorems in statistics but still get clear picture of what the equation is doing. I do not know how to prove central limit theorem mathematically but I can generate thousands of samples in R and see the distribution of sample means and variance which will be very close to the one given by the theorems. This approach helped me a lot to learn both statistics and R. So I want to share my notes with everyone who wants to learn statistics with minimum mathematics. All my notes on category of `Statistics` are my attempts to explain each topics with as much experiment as possible. 1. Principle of inference. 2. Mean and Variance 3. Permutation and combination 4. Probability theory 5. Statistical distribution 5.1 Binomial distribution 5.2 Normal Distribution (t-distribution $$\\\\chi^2$$ F- distribution) 6. Hypothesis testing 7. ANOVA (One-way and Factorial) 8. Regression (Simple multiple) 9. Degree of freedom type id_R project_name Visualization in R project_excerpt Re-visualize some famous plots using ggplot2 img :R.svg img_title img Rstudio date 2020-04-23 post # GGplot2 for visualization Data visualization can be done by using multiple tools like excel ggplot js or other. GGplot is one of the most flexible and powerful tool. type id_python project_name Learning Python for R users project_excerpt Tidy way to learn python img :python.svg img_title img python date 2021-05-27 post # Background Different people start learning to code at different time point in there career with different goals. I myself had started coding because ... type id_bioinformatics project_name Bioinformatics projects project_excerpt DGCA MEGENA and rna seq img :bioinformatics.svg img_title img bioinformatics date 2021-06-08 post These are the there projects I am working on now. 1. DGCA(../posts/2022-07-01-html_insidemd) 2. MEGENA 3. EDGER"
  },{
    "title"    : "Links"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/links.html"
    ,"date"     : "Jul 18, 2022"
    ,"content"  : "main header Some useful learning sources info This is the collection of some of my favourite resources my first and last resort. I am grateful to all the contributors in these websites category title Bioinformatics type id_bioinformatics color #800020 title JekyII / Liquid type id_jekyiiliquid color gray title Web Design type id_webdesign color #F4A273 title Programming type id_programming color #62b462 list nil type id_programming title Stack OverFlow url https://stackoverflow.com/ info Stack Overflow is a question and answer website for professional and enthusiastic programmers. type id_jekyiiliquid title Jekyll url https://jekyllrb.com/ info Transform your plain text into static websites and blogs. type id_jekyiiliquid title Jekyll Cheat Sheet url https://cloudcannon.com/community/jekyll-cheat-sheet/ info There are so many Jekyll variables and filters to remember and it can be tricky to keep it all in your head. This cheat sheet serves as a quick reference of everything Jekyll can do. type id_jekyiiliquid title Liquid for Designers url https://github.com/Shopify/liquid/wiki/Liquid-for-Designers info Liquid for Designers wiki on GitHub type id_jekyiiliquid title Liquid for Programmers url https://github.com/Shopify/liquid/wiki/Liquid-for-Programmers info Liquid for Programmers wiki on GitHub type id_jekyiiliquid title Liquid Reference url https://shopify.dev/api/liquid/ info Liquid is a template language created by Shopify and written in Ruby. It is now available as an open source project on GitHub type id_webdesign title W3Schools url https://www.w3schools.com/ info W3Schools offers free online tutorials references and exercises in all the major languages of the web. Covering popular subjects like HTML CSS JavaScript Python SQL Java and many more. type id_bioinformatics title Harvard course url https://liulab-dfci.github.io/bioinfo-combio/ info Collection of tutorials and videos from multiple instructors."
  },{
    "title"    : "Amrit Koirala"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/"
    ,"date"     : "Jul 18, 2022"
    ,"content"  : "  Data science has become a fad now-a-days and has gained a lot of attention from new generation. Because of rapid switch from the analogous to the digital world, every data is stored in an electronic form now. Every tweet we share, photos uploaded to social media, transactions made online, records of temperature at every corner of the world, stock exchange data, and demographic data from census are all stored online or offline in some form of electronic media in the form of 0s and 1s. This huge collection of data is both the mother and the operand of data science. The sole goal of data science is to be able to manage this huge collection of data (big data), recognize the patterns which might explain the question of interest and visualize the patterns to effectively communicate the observation. This huge collection of data is both the mother and the operand of data science. Source : NCBI stat   Biology and human health science have also made a significant dive into the digital data pool. As shown by the NCBI Gene Bank data above, sequencing data size has sky rocketed in last two decades. In addition to improvement in sequencing technology, spectrometric and imaging techniques have also been polished adding huge amount of data of various kinds like images, videos and 3D structures of proteins. To understand these big data in biology it is essential to make the optimum use of knowledge from diverse filed like statistics, mathematics and computer science. This web site is the online record of my journey to understand and analyze biological data using tools available in various programming languages like R, Python, and bash. Here you will find notes on various topics on Statistics, R, Python, bash scripting and miscellaneous topics on latest biological and bioinformatics techniques. Hope you will enjoy coming back here. ✍"
  },{
    "title"    : "About"
    ,"category" : ""
    ,"tags"     : ""
    ,"url"      : "/tabs/about.html"
    ,"date"     : "Jul 18, 2022"
    ,"content"  : "“The only true wisdom is in knowing you know nothing.”                                 - Socrates   I am a bioinformatician by profession working at Baylor College of Medicine, Houston, TX. During my PhD research at SDSU, I realized the enormity of sequencing data and the potential knowledge that could be mined from the publicly available databases like Genebank and Uniprot. This ignited my interest into the bioinformatics and eventually I completed master’s in data science to get the firm grasp of cutting-edge techniques in bigdata management and machine learning. I have an extensive experience working with genomic data, phylogenetic analysis, gene expression data and other omics. R and python are my favorite computer languages and I like learning and teaching new statistical methods specialized for the study of multiple -omics data in biology. Area of expertise   Data science being an inter-disciplinary subject needs expertise in range of fields. I would like to delineate my area of interest and expertise burrowing the concept of data science venn diagram by Drew Conway. Education South Dakota State University, SD, USA Doctor of Philosophy - PhD, Microbiology, General 2016 - 2021 Master’s Degree, Data Science. Programme ranked second best by Intelligent. 2018 - 2020 Tribhuvan University, Kathmandu, Nepal Masters of Science, Medical Microbiology 2012 -2015 Bachelor’s Degree in Science, Microbiology 2009 - 2012 Publications google scholar Koirala, Amrit. 2021. “Free-Living Diazotrophs and the Nitrogen Cycle in Natural Grassland Revealed by Culture Dependent and Independent Approaches.” PhD thesis, South Dakota State University. Agrahari, Gaurav, Amrit Koirala, Roshan Thapa, Mahesh Kumar Chaudhary, and Reshma Tuladhar. 2019. “Antimicrobial Resistance Patterns and Plasmid Profiles of Methicillin Resistant Staphylococcus Aureus Isolated from Clinical Samples.” Nepal Journal of Biotechnology 7 (1): 8–14. Koirala, Amrit, Gaurav Agrahari, Nabaraj Dahal, Prakash Ghimire, and KR Rijal. 2017. “ESBL and MBL Mediated Resistance in Clinical Isolates of Nonfermentating Gram Negative Bacilli (NFGNB) in Nepal.” J Microb Antimicrob Agents 3 (1): 18–24. Koirala, Amrit, and Volker S Brözel. 2021. “Phylogeny of Nitrogenase Structural and Assembly Components Reveals New Insights into the Origin and Distribution of Nitrogen Fixation Across Bacteria and Archaea.” Microorganisms 9 (8): 1662. Tamang, Kamal, Pashupati Shrestha, Amrit Koirala, Jagat Khadka, Narayan Gautam, and Komal Raj Rijal. 2018. “Prevalence of Bacterial Uropathogens Among Diabetic Patients Attending Padma Nursing Hospital of Western Nepal.”"
  }
  ]

